<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Apache Paimon -- 文件操作</title>
    <link href="/2023/10/18/paimon/Apache%20Paimon%20--%20%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/"/>
    <url>/2023/10/18/paimon/Apache%20Paimon%20--%20%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="创建-Catalog"><a href="#创建-Catalog" class="headerlink" title="创建 Catalog"></a>创建 Catalog</h2><p>在 Flink lib 中放⼊ <a href="https://repository.apache.org/snapshots/org/apache/paimon/paimon-flink-1.16/0.6-SNAPSHOT/">paimon-flink 依赖包</a>，执⾏ .&#x2F;sql-client.sh 启动 Flink SQL Client，然后执⾏下⾯的命令去创建 Paimon catlog：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> CATALOG paimon <span class="hljs-keyword">WITH</span> (<br>    <span class="hljs-string">&#x27;type&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;paimon&#x27;</span>,<br>    <span class="hljs-string">&#x27;warehouse&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;file:///tmp/paimon&#x27;</span><br>);<br><br>USE CATALOG paimon;<br></code></pre></td></tr></table></figure><p>执⾏完后会在 file:&#x2F;&#x2F;&#x2F;tmp&#x2F;paimon 路径下创建⽬录 default.db。</p><h2 id="创建-Table"><a href="#创建-Table" class="headerlink" title="创建 Table"></a>创建 Table</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> T<br>(<br>    id <span class="hljs-type">BIGINT</span>,<br>    a  <span class="hljs-type">INT</span>,<br>    b  STRING,<br>    dt STRING COMMENT <span class="hljs-string">&#x27;timestamp string in format yyyyMMdd&#x27;</span>,<br>    <span class="hljs-keyword">PRIMARY</span> KEY (id, dt) <span class="hljs-keyword">NOT</span> ENFORCED<br>) PARTITIONED <span class="hljs-keyword">BY</span> (dt);<br></code></pre></td></tr></table></figure><p>执⾏后，Paimon 表 T 会在 &#x2F;tmp&#x2F;paimon&#x2F;default.db&#x2F;T ⽬录下⽣成⽬录，它的 schema 会存放在⽬录 &#x2F;tmp&#x2F;paimon&#x2F;default.db&#x2F;T&#x2F;schema&#x2F;schema-0 下。</p><h2 id="写入数据到-Table"><a href="#写入数据到-Table" class="headerlink" title="写入数据到 Table"></a>写入数据到 Table</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> T <span class="hljs-keyword">VALUES</span> (<span class="hljs-number">1</span>, <span class="hljs-number">10001</span>, <span class="hljs-string">&#x27;varchar00001&#x27;</span>, <span class="hljs-string">&#x27;20230501&#x27;</span>);<br></code></pre></td></tr></table></figure><p>⽤户可以通过执⾏查询 <code>SELECT * FROM T;</code> 来验证这些记录的可⻅性，该查询将返回⼀⾏结果。</p><p>提交过程会创建⼀个位于路径 &#x2F;tmp&#x2F;paimon&#x2F;default.db&#x2F;T&#x2F;snapshot&#x2F;snapshot-1 的快照。snapshot-1 的⽂件布局如下所述：</p><p>⼀旦任务运⾏完成变成 finished 时，提交成功后数据就写⼊到 Paimon 表中。⽤户可以通过执⾏查询 SELECT * FROM T 来验证数据的可⻅性，该查询将返回⼀⾏结果。</p><p>另外可以发现⽬录的结构发⽣如下变化，新增 <code>dt=20230501</code> 、 <code>manifest</code> 和 <code>snapshot</code> 三个⽬录。三个的⽬录结构如下：<br><img src="/img/paimon/job/paimon-job-3.png"></p><p>查询⽬录下的数据如下所示：<br><img src="/img/paimon/job/paimon-job-4.png"></p><p><img src="/img/paimon/job/paimon-job-5.png"></p><p>这是因为提交过程中会创建⼀个位于 &#x2F;tmp&#x2F;paimon&#x2F;default.db&#x2F;T&#x2F;snapshot&#x2F;snapshot-1 的快照。 snapshot-1 的⽂件布局如下所述：<br><img src="/img/paimon/job/paimon-job-6.png"></p><p>snapshot-1 的内容包含了这个 snapshot 的元数据，⽐如 manifest list 和 schema id：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;version&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;id&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;schemaId&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;baseManifestList&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;manifest-list-9c408d6b-21cf-445d-9be8-e3f3b04f1bc4-0&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;deltaManifestList&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;manifest-list-9c408d6b-21cf-445d-9be8-e3f3b04f1bc4-1&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;changelogManifestList&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">null</span></span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;commitUser&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;76198586-8c3f-4b4e-ae8a-9ffc5c871965&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;commitIdentifier&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">9223372036854775807</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;commitKind&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;APPEND&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;timeMillis&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">1697509431254</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;logOffsets&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span> <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;totalRecordCount&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;deltaRecordCount&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;changelogRecordCount&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;watermark&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">-9223372036854775808</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>需要提醒的是，manifest list 包含了 snapshot 的所有更改，baseManifestList 是应⽤在 deltaManifestList 中的更改所基于的基本⽂件。第⼀次提交将导致⽣成 1 个清单⽂件，并创建了 2 个清单列表（⽂件名可能与你的实验中的不同）：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">-rw-r--r--   3 flink supergroup       1714 2023-10-17 10:23 /paimon/default.db/T/manifest/manifest-00ab8926-acc2-4e31-b6b1-776e00976dc1-0<br>-rw-r--r--   3 flink supergroup        676 2023-10-17 10:23 /paimon/default.db/T/manifest/manifest-list-9c408d6b-21cf-445d-9be8-e3f3b04f1bc4-0<br>-rw-r--r--   3 flink supergroup        794 2023-10-17 10:23 /paimon/default.db/T/manifest/manifest-list-9c408d6b-21cf-445d-9be8-e3f3b04f1bc4-1<br></code></pre></td></tr></table></figure><p><img src="/img/paimon/job/paimon-job-7.png"></p><p><code>manifest-00ab8926-acc2-4e31-b6b1-776e00976dc1-0</code>：如前⾯⽂件布局图中的 manifest-1-0 ，存储了关于 snapshot 中数据⽂件信息的 manifest。<br><img src="/img/paimon/job/paimon-job-8.png"></p><p><code>manifest-list-9c408d6b-21cf-445d-9be8-e3f3b04f1bc4-0</code>：是基础的 baseManifestList，如前⾯⽂件布局图中的 manifest-list-1-base ，实际上是空的。<br><img src="/img/paimon/job/paimon-job-9.png"></p><p><code>manifest-list-9c408d6b-21cf-445d-9be8-e3f3b04f1bc4-1</code>：是deltaManifestList，如前⾯⽂件布局图中的 manifest-list-1-delta ，其中包含⼀系列对数据⽂件执⾏操作的清单条⽬， ⽽在这种情况下，清单条⽬为 manifest-1-0。</p><p>在不同分区中插⼊⼀批记录，在 Flink SQL 中执⾏以下语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> T <span class="hljs-keyword">VALUES</span><br>(<span class="hljs-number">2</span>, <span class="hljs-number">10002</span>, <span class="hljs-string">&#x27;varchar00002&#x27;</span>, <span class="hljs-string">&#x27;20230502&#x27;</span>),<br>(<span class="hljs-number">3</span>, <span class="hljs-number">10003</span>, <span class="hljs-string">&#x27;varchar00003&#x27;</span>, <span class="hljs-string">&#x27;20230503&#x27;</span>),<br>(<span class="hljs-number">4</span>, <span class="hljs-number">10004</span>, <span class="hljs-string">&#x27;varchar00004&#x27;</span>, <span class="hljs-string">&#x27;20230504&#x27;</span>),<br>(<span class="hljs-number">5</span>, <span class="hljs-number">10005</span>, <span class="hljs-string">&#x27;varchar00005&#x27;</span>, <span class="hljs-string">&#x27;20230505&#x27;</span>),<br>(<span class="hljs-number">6</span>, <span class="hljs-number">10006</span>, <span class="hljs-string">&#x27;varchar00006&#x27;</span>, <span class="hljs-string">&#x27;20230506&#x27;</span>),<br>(<span class="hljs-number">7</span>, <span class="hljs-number">10007</span>, <span class="hljs-string">&#x27;varchar00007&#x27;</span>, <span class="hljs-string">&#x27;20230507&#x27;</span>),<br>(<span class="hljs-number">8</span>, <span class="hljs-number">10008</span>, <span class="hljs-string">&#x27;varchar00008&#x27;</span>, <span class="hljs-string">&#x27;20230508&#x27;</span>),<br>(<span class="hljs-number">9</span>, <span class="hljs-number">10009</span>, <span class="hljs-string">&#x27;varchar00009&#x27;</span>, <span class="hljs-string">&#x27;20230509&#x27;</span>),<br>(<span class="hljs-number">10</span>, <span class="hljs-number">10010</span>, <span class="hljs-string">&#x27;varchar00010&#x27;</span>, <span class="hljs-string">&#x27;20230510&#x27;</span>);<br></code></pre></td></tr></table></figure><p><img src="/img/paimon/job/paimon-job-10.png"></p><p>等任务执⾏完成并提交快照后，执⾏ SELECT * FROM T 将返回 10 ⾏数据。<br><img src="/img/paimon/job/paimon-job-11.png"></p><p><img src="/img/paimon/job/paimon-job-12.png"></p><p>创建了⼀个新的快照，即 snapshot-2 ，并给出了以下物理⽂件布局：<br><img src="/img/paimon/job/paimon-job-13.png"></p><p>新的快照⽂件布局图如下：<br><img src="/img/paimon/job/paimon-job-14.png"></p><h3 id="删除数据"><a href="#删除数据" class="headerlink" title="删除数据"></a>删除数据</h3><p>不同的 flink 版本操作方式不一致：<a href="https://paimon.apache.org/docs/master/how-to/writing-tables/#deleting-from-table">Deleting from table</a></p><p>这里采用 flink 1.17，接下来删除满⾜条件 dt&gt;&#x3D;20230503 的数据。在 Flink SQL Client 中，执⾏以下语句：</p><p>需要设置 <code>SET execution.runtime-mode = &#39;batch&#39;</code> 再执行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">DELETE</span> <span class="hljs-keyword">FROM</span> T <span class="hljs-keyword">WHERE</span> dt <span class="hljs-operator">&gt;=</span> <span class="hljs-string">&#x27;20230503&#x27;</span>;<br></code></pre></td></tr></table></figure><p><img src="/img/paimon/job/paimon-job-15.png"></p><p>第三次提交完成，并⽣成了快照 snapshot-3 。现在，表下的⽬录，会发现没有分区被删除。相反，为分区 20230503 到 20230510 创建了⼀个新的数据⽂件：</p><p><img src="/img/paimon/job/paimon-job-16.png"></p><p>这是有道理的，因为我们在第⼆次提交中插⼊了⼀条数据（为 +I[10, 10010, ‘varchar00010’, ‘20230510’] ），然后在第三次提交中删除了数据。现在再执⾏⼀下 SELECT * FROM T 只能查询到两条数据。</p><p><img src="/img/paimon/job/paimon-job-17.png"></p><p>snapshot-3 后新的⽂件布局如下图：</p><p><img src="/img/paimon/job/paimon-job-18.png"></p><p><img src="/img/paimon/job/paimon-job-19.png"></p><p><img src="/img/paimon/job/paimon-job-20.png"></p><p>请注意， manifest-3-0 包含了 8 个 ADD 操作类型的清单条⽬，对应着 8 个新写⼊的数据⽂件。</p><p><img src="/img/paimon/job/paimon-job-21.png"></p><h2 id="Compact-Table"><a href="#Compact-Table" class="headerlink" title="Compact Table"></a>Compact Table</h2><p>你可能已经注意到的，随着连续 snapshot 的增加，⼩⽂件的数量会增加，这可能会导致读取性能下降。因此，需要进⾏全量压缩以减少⼩⽂件的数量。</p><p>通过 flink run 运⾏⼀个专⽤的<a href="https://repository.apache.org/snapshots/org/apache/paimon/paimon-flink-action/0.6-SNAPSHOT/">合并⼩⽂件任务</a>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">&lt;FLINK_HOME&gt;/bin/flink run \<br> -D execution.runtime-mode=batch \<br> ./paimon-flink-action-0.6-SNAPSHOT.jar \<br> compact \<br> --warehouse &lt;warehouse-path&gt; \<br> --database &lt;database-name&gt; \<br> --table &lt;table-name&gt; \<br> [--partition &lt;partition-name&gt;] \<br> [--catalog-conf &lt;paimon-catalog-conf&gt; [--catalog-conf &lt;paimon-catalogconf&gt; ...]] \<br> [--table-conf &lt;paimon-table-dynamic-conf&gt; [--table-conf &lt;paimon-tabledynamic-conf&gt;] ...]<br></code></pre></td></tr></table></figure><p>例如（提前下载好压缩任务 jar 在 Flink 客户端下）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">./flink run \<br> -D execution.runtime-mode=batch \<br> /Users/liujun/flink/flink-1.17.1/lib/paimon-flink-action-0.5.0-incubating.jar \<br> compact \<br> --path file:///tmp/paimon/default.db/T<br></code></pre></td></tr></table></figure><p><img src="/img/paimon/job/paimon-job-22.png"></p><p><img src="/img/paimon/job/paimon-job-23.png"></p><p>压缩任务结束后，再来看下 snapshot-4 ⽂件结构：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;version&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;id&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;schemaId&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;baseManifestList&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;manifest-list-4b0018e9-435c-4bfb-b379-6b918aa75c48-0&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;deltaManifestList&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;manifest-list-4b0018e9-435c-4bfb-b379-6b918aa75c48-1&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;changelogManifestList&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">null</span></span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;commitUser&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;722fd008-f240-4225-ae2b-12b1a1c5f546&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;commitIdentifier&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">9223372036854775807</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;commitKind&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;COMPACT&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;timeMillis&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">1697591820188</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;logOffsets&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span> <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;totalRecordCount&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">38</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;deltaRecordCount&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">20</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;changelogRecordCount&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;watermark&quot;</span> <span class="hljs-punctuation">:</span> <span class="hljs-number">-9223372036854775808</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p><img src="/img/paimon/job/paimon-job-24.png"></p><p><img src="/img/paimon/job/paimon-job-25.png"></p><p>manifest-4-0 包含 20 个清单条⽬（18 个 DELETE 操作和 2 个 ADD 操作）:</p><ul><li>对于分区 20230503 到 20230510，有两个删除操作，对应两个数据⽂件</li><li>对于分区 20230501 到 20230502，有⼀个删除操作和⼀个添加操作，对应同⼀个数据⽂件。</li></ul><h2 id="Alter-Table"><a href="#Alter-Table" class="headerlink" title="Alter Table"></a>Alter Table</h2><p>执⾏以下语句来配置全量压缩：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> T <span class="hljs-keyword">SET</span> (<span class="hljs-string">&#x27;full-compaction.delta-commits&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;1&#x27;</span>);<br></code></pre></td></tr></table></figure><p>这将为 Paimon 表创建⼀个新的 schema，即 schema-1，但在下⼀次提交之前，不会有任何 snapshot 使⽤此模式。</p><p><img src="/img/paimon/job/paimon-job-26.png"></p><h2 id="过期-snapshot"><a href="#过期-snapshot" class="headerlink" title="过期 snapshot"></a>过期 snapshot</h2><p>请注意，标记为删除的数据⽂件直到 snapshot 过期且没有任何消费者依赖于该 snapshot 时才会真正被删除。参考 <a href="https://paimon.apache.org/docs/master/maintenance/manage-snapshots/#expiring-snapshots">Manage Snapshots</a> 可以查阅更多信息。</p><p>在 snapshot 过期过程中，⾸先确定 snapshot 的范围，然后标记这些 snapshot 内的数据⽂件以进⾏删除。只有当存在引⽤特定数据⽂件的 DELETE 类型的清单条⽬时，才会标记该数据⽂件进⾏删除。这种标记确保⽂件不会被后续的 snapshot 使⽤，并且可以安全地删除。</p><p>假设上图中的所有 4 个 snapshot 即将过期。过期过程如下:</p><p>1、⾸先删除所有标记为删除的数据⽂件，并记录任何更改的 bucket。</p><p>2、然后删除所有的 changelog ⽂件和关联的 manifests。</p><p>3、最后，删除快照本身并写⼊最早的提示⽂件。</p><p>如果删除过程后留下的空⽬录，也将被删除。</p><p>假设创建了另⼀个快照 snapshot-5，并触发了快照过期。将删除 snapshot-1 到 snapshot-4。为简单起⻅，我们只关注以前快照的⽂件，快照过期后的最终布局如下：</p><p><img src="/img/paimon/job/paimon-job-27.png"></p><p>因此，分区 20230503 到 20230510 的数据被物理删除了。</p><h2 id="Flink-流式写⼊"><a href="#Flink-流式写⼊" class="headerlink" title="Flink 流式写⼊"></a>Flink 流式写⼊</h2><p>我们通过利⽤ CDC 数据的示例来测试 Flink 流式写⼊，将介绍将变更数据捕获并写⼊ Paimon 的过程， 以及异步压缩、快照提交和过期的机制背后的原理。</p><p>帮我们更详细地了解 CDC 数据摄取的⼯作流程以及每个参与组件所扮演的独特⻆⾊。<br><img src="/img/paimon/job/paimon-job-28.png"></p><p>1、MySQL CDC Source 统⼀读取快照数据和增量数据，其中 SnapshotReader 读取快照数据，⽽ BinlogReader 读取增量数据。</p><p>2、Paimon Sink 将数据按 Bucket 级别写⼊ Paimon 表中。其中的 CompactManager 将异步触发压缩操作。</p><p>3、Committer Operator 是⼀个单例，负责提交和过期快照。</p><p>接下来，我们将逐步介绍端到端的数据流程：<br><img src="/img/paimon/job/paimon-job-29.png"></p><p>MySQL CDC Source ⾸先读取快照数据和增量数据，然后对它们进⾏规范化处理，并将其发送到下游。<br><img src="/img/paimon/job/paimon-job-30.png"></p><p>Paimon Sink ⾸先将新记录缓存在基于堆的 LSM 树中，并在内存缓冲区满时将其 flush 到磁盘上。 请注意，每个写⼊的数据⽂件都是⼀个 sorted run。在这个阶段，还没有创建 manifest ⽂件和 snapshot。</p><p>在 Flink 执⾏ Checkpoint 之前，Paimon Sink 将 flush 所有缓冲的记录并发送可提交的消息到下游，下游会在 Checkpoint 期间由 Committer Operator 读取并提交。<br><img src="/img/paimon/job/paimon-job-31.png"></p><p>在 Checkpoint 期间，Committer Operator 将创建⼀个新的 snapshot，并将其与 manifest lists 关联，以便snapshot 包含表中所有数据⽂件的信息。</p><p>稍等⼀会后，可能会进⾏异步压缩，CompactManager ⽣成的可提交消息包含有关先前⽂件和合并⽂件的信息，以便 Committer Operator 可以构建相应的 manifest entries。</p><p>在这种情况下，Committer Operator 在Flink Checkpoint 期间可能会⽣成两个 snapshot，⼀个⽤于写⼊的数据（类型为 Append 的快照），另⼀个⽤于压缩（类型为 Compact 的快照）。</p><p>如果在 Checkpoint 间隔期间没有写⼊数据⽂件，则只会创建类型为Compact 的快照。 Committer Operator 将检查 snapshot 的过期情况，并对标记为删除的数据⽂件执⾏物理删除操作。</p>]]></content>
    
    
    <categories>
      
      <category>Apache Paimon</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apache Paimon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Paimon -- 基本概念</title>
    <link href="/2023/10/17/paimon/Apache%20Paimon%20--%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <url>/2023/10/17/paimon/Apache%20Paimon%20--%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="1-Snapshot"><a href="#1-Snapshot" class="headerlink" title="1 Snapshot"></a>1 Snapshot</h3><p>快照（Snapshot）是在某个时间点上捕捉表状态的⽅式。⽤户可以通过最新的快照访问表的最新数据。 通过时间回溯，⽤户还可以通过较早的快照访问表的先前状态。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Snapshot</span> &#123;<br><br>    <span class="hljs-comment">// ... ...</span><br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_VERSION</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;version&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_ID</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;id&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_SCHEMA_ID</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;schemaId&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_BASE_MANIFEST_LIST</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;baseManifestList&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_DELTA_MANIFEST_LIST</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;deltaManifestList&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_CHANGELOG_MANIFEST_LIST</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;changelogManifestList&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_INDEX_MANIFEST</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;indexManifest&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_COMMIT_USER</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;commitUser&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_COMMIT_IDENTIFIER</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;commitIdentifier&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_COMMIT_KIND</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;commitKind&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_TIME_MILLIS</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;timeMillis&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_LOG_OFFSETS</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;logOffsets&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_TOTAL_RECORD_COUNT</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;totalRecordCount&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_DELTA_RECORD_COUNT</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;deltaRecordCount&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_CHANGELOG_RECORD_COUNT</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;changelogRecordCount&quot;</span>;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">FIELD_WATERMARK</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;watermark&quot;</span>;<br><br>    <span class="hljs-comment">// ... ...</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="2-Partition"><a href="#2-Partition" class="headerlink" title="2 Partition"></a>2 Partition</h3><p>Paimon 采用与 Apache Hive 相同的分区概念来分离数据。</p><p>分区是一种可选方法，可根据日期、城市和部门等特定列的值将表划分为相关部分。每个表可以有一个或多个分区键来标识特定分区。</p><p>通过分区，用户可以高效地操作表中的一片记录。有关如何将文件划分为多个分区的信息，请参阅<a href="https://paimon.apache.org/docs/master/concepts/file-layouts/">文件布局</a>。</p><blockquote><p>如果定义了主键，分区键必须是主键的⼦集。</p></blockquote><h3 id="3-Bucket"><a href="#3-Bucket" class="headerlink" title="3 Bucket"></a>3 Bucket</h3><p>未分区表或分区表中的分区被细分为存储桶，以便为可用于更有效查询的数据提供额外的结构。</p><p>桶的范围由记录中的一列或多列的哈希值确定。用户可以通过提供bucket-key选项来指定分桶列。如果未<a href="https://paimon.apache.org/docs/master/maintenance/configurations/#coreoptions">bucket-key指定</a>选项，则主键（如果已定义）或完整记录将用作存储桶键。</p><p>桶是读写的最小存储单元，因此桶的数量限制了最大处理并行度。不过这个数字不应该太大，因为它会导致大量小文件和低读取性能。一般来说，建议每个桶的数据大小为1GB左右。</p><p>请参阅<a href="https://paimon.apache.org/docs/master/concepts/file-layouts/">文件布局</a>了解文件如何划分到存储桶中。另外，如果您想在创建表后调整存储桶的数量，请参阅<a href="https://paimon.apache.org/docs/master/maintenance/rescale-bucket/">重新缩放存储桶</a>。</p><h3 id="4-一致性保证"><a href="#4-一致性保证" class="headerlink" title="4 一致性保证"></a>4 一致性保证</h3><p>Paimon 编写器使用两阶段提交协议以原子方式将一批记录提交到表中。每次提交在提交时最多生成两个<a href="https://paimon.apache.org/docs/master/concepts/basic-concepts/#snapshot">快照</a>。</p><p>对于任意两个同时修改表的写入者，只要他们不修改同一个存储桶，他们的提交就可以并行发生。如果他们修改同一个存储桶，则仅保证快照隔离。也就是说，最终表状态可能是两次提交的混合，但不会丢失任何更改。</p>]]></content>
    
    
    <categories>
      
      <category>Apache Paimon</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apache Paimon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Paimon -- 文件布局</title>
    <link href="/2023/10/17/paimon/Apache%20Paimon%20--%20%E6%96%87%E4%BB%B6%E5%B8%83%E5%B1%80/"/>
    <url>/2023/10/17/paimon/Apache%20Paimon%20--%20%E6%96%87%E4%BB%B6%E5%B8%83%E5%B1%80/</url>
    
    <content type="html"><![CDATA[<p>一张表的所有文件都存储在一个基本目录下。Paimon 文件以分层方式组织。下图说明了文件布局。从快照文件开始，Paimon<br>读者可以递归地访问表中的所有记录。<br><img src="/img/paimon/paimon-file-1.png"></p><h2 id="1-Snapshot-Files"><a href="#1-Snapshot-Files" class="headerlink" title="1 Snapshot Files"></a>1 Snapshot Files</h2><p>所有的 snapshot ⽂件都存储在 snapshot ⽬录下，snapshot file 是⼀个包含了 snapshot 信息的 JSON ⽂件：</p><ul><li>使⽤的 Schema ⽂件；</li><li>manifest 列表包含了 snapshot 的所有变更。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Snapshot</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Integer version;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> id;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> schemaId;<br>    <span class="hljs-comment">// a manifest list recording all changes from the previous snapshots</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String baseManifestList;<br>    <span class="hljs-comment">// a manifest list recording all new changes occurred in this snapshot</span><br>    <span class="hljs-comment">// for faster expire and streaming reads</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String deltaManifestList;<br>    <span class="hljs-comment">// a manifest list recording all changelog produced in this snapshot</span><br>    <span class="hljs-comment">// null if no changelog is produced, or for paimon &lt;= 0.2</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String changelogManifestList;<br>    <span class="hljs-comment">// a manifest recording all index files of this table</span><br>    <span class="hljs-comment">// null if no index file</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String indexManifest;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String commitUser;<br>    <span class="hljs-comment">// Mainly for snapshot deduplication.</span><br>    <span class="hljs-comment">//</span><br>    <span class="hljs-comment">// If multiple snapshots have the same commitIdentifier, reading from any of these snapshots</span><br>    <span class="hljs-comment">// must produce the same table.</span><br>    <span class="hljs-comment">//</span><br>    <span class="hljs-comment">// If snapshot A has a smaller commitIdentifier than snapshot B, then snapshot A must be</span><br>    <span class="hljs-comment">// committed before snapshot B, and thus snapshot A must contain older records than snapshot B.</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> commitIdentifier;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> CommitKind commitKind;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> timeMillis;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Map&lt;Integer, Long&gt; logOffsets;<br>    <span class="hljs-comment">// record count of all changes occurred in this snapshot</span><br>    <span class="hljs-comment">// null for paimon &lt;= 0.3</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Long totalRecordCount;<br>    <span class="hljs-comment">// record count of all new changes occurred in this snapshot</span><br>    <span class="hljs-comment">// null for paimon &lt;= 0.3</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Long deltaRecordCount;<br>    <span class="hljs-comment">// record count of all changelog produced in this snapshot</span><br>    <span class="hljs-comment">// null for paimon &lt;= 0.3</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Long changelogRecordCount;<br>    <span class="hljs-comment">// watermark for input records</span><br>    <span class="hljs-comment">// null for paimon &lt;= 0.3</span><br>    <span class="hljs-comment">// null if there is no watermark in new committing, and the previous snapshot does not have a</span><br>    <span class="hljs-comment">// watermark</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Long watermark;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="2-Manifest-Files"><a href="#2-Manifest-Files" class="headerlink" title="2 Manifest Files"></a>2 Manifest Files</h2><ul><li><p>所有的 manifest lists 和 manifest ⽂件都存放在 manifest ⽬录下。</p></li><li><p>manifest list 是⼀组 manifest ⽂件名列表。</p></li><li><p>manifest ⽂件是⼀个包含有关 LSM 数据⽂件和变更⽇志⽂件的变更信息的⽂件。例如，它记录了在对应的快照中创建了哪个 LSM 数据⽂件以及删除了哪个⽂件。</p></li></ul><p>Schema:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Schema</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> List&lt;DataField&gt; fields;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> List&lt;String&gt; partitionKeys;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> List&lt;String&gt; primaryKeys;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Map&lt;String, String&gt; options;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String comment;<br>&#125;<br></code></pre></td></tr></table></figure><p>FileKind:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">enum</span> <span class="hljs-title class_">FileKind</span> &#123;<br>    ADD((<span class="hljs-type">byte</span>) <span class="hljs-number">0</span>),<br><br>    DELETE((<span class="hljs-type">byte</span>) <span class="hljs-number">1</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>IndexFileMeta:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">IndexFileMeta</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String indexType;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String fileName;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> fileSize;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> rowCount;<br>&#125;<br></code></pre></td></tr></table></figure><p>IndexManifestEntry:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">IndexManifestEntry</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> FileKind kind;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> BinaryRow partition;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> bucket;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> IndexFileMeta indexFile;<br>&#125;<br></code></pre></td></tr></table></figure><p>ManifestFileMeta:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ManifestFileMeta</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String fileName;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> fileSize;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> numAddedFiles;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> numDeletedFiles;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> BinaryTableStats partitionStats;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> schemaId;<br>&#125;<br></code></pre></td></tr></table></figure><p>ManifestCommittable:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ManifestCommittable</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> identifier;<br>    <span class="hljs-meta">@Nullable</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Long watermark;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Map&lt;Integer, Long&gt; logOffsets;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> List&lt;CommitMessage&gt; commitMessages;<br>&#125;<br></code></pre></td></tr></table></figure><p>ManifestFile:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 有 write ⽅法将各种 ManifestEntry 写进去 ManifestFile，其中会统计对应的 metadata</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ManifestFile</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">ObjectsFile</span>&lt;ManifestEntry&gt; &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> SchemaManager schemaManager;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> RowType partitionType;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> FormatWriterFactory writerFactory;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> suggestedFileSize;<br>&#125;<br></code></pre></td></tr></table></figure><p>ManifestList:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// This file includes several ManifestFileMeta, representing all data of the whole table at the corresponding snapshot.</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ManifestList</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">ObjectsFile</span>&lt;ManifestFileMeta&gt; &#123;<br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">write</span><span class="hljs-params">(List&lt;ManifestFileMeta&gt; metas)</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>.writeWithoutRolling(metas);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="3-Data-Files"><a href="#3-Data-Files" class="headerlink" title="3 Data Files"></a>3 Data Files</h2><p>数据⽂件按照分区和 bucket 进⾏分组。每个 bucket ⽬录包含⼀个 LSM 树和其对应的变更⽇志⽂件。<br>⽬前，Paimon ⽀持使⽤ orc（默认）、parquet 和 avro 作为数据⽂件的格式。</p><h2 id="4-LSM-Trees"><a href="#4-LSM-Trees" class="headerlink" title="4 LSM Trees"></a>4 LSM Trees</h2><p>Paimon 采⽤ LSM 树（⽇志结构合并树）作为⽂件存储的数据结构。下⾯简要介绍了关于 LSM 树的概念。</p><h3 id="Sorted-Runs"><a href="#Sorted-Runs" class="headerlink" title="Sorted Runs"></a>Sorted Runs</h3><p>LSM 树将⽂件组织成多个 sorted runs。⼀个 sorted run 由⼀个或多个数据⽂件组成，每个数据⽂件都属于且只属于⼀个 sorted run。</p><p>数据⽂件内的记录按其主键进⾏排序。在⼀个 sorted run 内，数据⽂件的主键范围不会重叠。</p><p><img src="/img/paimon/paimon-file-2.png"></p><p>正如您所看到的，不同的 sorted run 可能具有重叠的主键范围，甚⾄可能包含相同的主键。在查询 LSM 树时，必须将所有的 sorted run 组合起来，并根据⽤户指定的合并引擎和每个记录的时间戳进⾏主键相同的记录合并。</p><p>写⼊ LSM 树的新记录将⾸先缓存在内存中。当内存缓冲区满时，所有内存中的记录将被排序并刷新到磁盘上。此时就会创建⼀个新的 sorted run。</p><h2 id="5-Compaction"><a href="#5-Compaction" class="headerlink" title="5 Compaction"></a>5 Compaction</h2><p>当越来越多的记录被写⼊ LSM 树时，sorted run 的数量会增加。因为查询 LSM 树需要将所有 sorted run 组合起来，过多的 sorted run 将导致查询性能下降，甚⾄可能导致内存不⾜。</p><p>为了限制 sorted run 的数量，我们需要定期将⼏个 sorted run 合并成⼀个⼤的 sorted run。这个过程被称为compaction。</p><p>然⽽，compaction 是⼀个资源密集型的过程，会消耗⼀定的 CPU 时间和磁盘 IO，因此过于频繁的 compaction 可能会导致写⼊速度变慢。这是查询性能和写⼊性能之间的权衡。Paimon ⽬前采⽤了类似 Rocksdb 的 <a href="https://github.com/facebook/rocksdb/wiki/Universal-Compaction">universal compaction</a> 策略。</p><p>默认情况下，当 Paimon 向 LSM 树追加记录时，会根据需要进⾏ compaction。⽤户也可以选择在<a href="https://paimon.apache.org/docs/0.5/maintenance/multiple-writers/#dedicated-compaction-job">单独的 compaction</a> 作业中执⾏所有的 compaction 操作。</p>]]></content>
    
    
    <categories>
      
      <category>Apache Paimon</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apache Paimon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Apache Paimon -- 介绍</title>
    <link href="/2023/10/17/paimon/Apache%20Paimon%20--%20%E4%BB%8B%E7%BB%8D/"/>
    <url>/2023/10/17/paimon/Apache%20Paimon%20--%20%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Flink-Table-Store-介绍"><a href="#1-Flink-Table-Store-介绍" class="headerlink" title="1 Flink Table Store 介绍"></a>1 Flink Table Store 介绍</h2><p>从 Flink Table Store 演进⽽来，架构图如下：</p><p><img src="/img/paimon/flink-table-store.png"></p><p>（和今天 Paimon 的架构相⽐，Log System 不再被推荐使⽤，Lake Store 的能⼒⼤幅强于 Log System，除了延时）</p><p>2021 年 9 ⽉，发布了 0.2 版本，陆续有在⽣产使⽤。</p><p>Flink Table Store 是⼀个数据湖存储，⽤于实时流式 Changelog 写⼊ (⽐如来⾃ Flink CDC 的数据) 和<br>⾼性能查询。它创新性的结合湖存储和 LSM 结构，深度对接 Flink，提供实时更新的系统设计，⽀撑⼤<br>吞吐量的更新数据摄取，同时提供良好的查询性能。</p><p>0.3 形成了⼀个 Streaming Lakehouse 的基本雏形，我们可以⽐较⾃信的说出，0.3 可以推荐⽣产可⽤了。</p><p>基于 Flink Table Store 不仅可以⽀持数据实时⼊湖，⽽且⽀持 Partial Update 等功能，帮助⽤户更灵活的在延迟和成本之间做均衡。<br><img src="/img/paimon/paimon-2.png"></p><p>解决问题：<br>1、Paimon 实时 CDC ⼊湖<br>2、Paimon 实时宽表与流读</p><h2 id="2-Apache-Paimon"><a href="#2-Apache-Paimon" class="headerlink" title="2 Apache Paimon"></a>2 Apache Paimon</h2><p>在发布了三个版本后，虽然 Flink Table Store 具备了⼀定的成熟度，但作为 Flink 社区的⼀个⼦项⽬，<br>在⽣态发展（⽐如 Spark ⽤户选择和使⽤）⽅⾯存在⽐较明显的局限性。为了让 Flink Table Store 能<br>够有更⼤的发展空间和⽣态体系， Flink PMC 经过讨论决定将其捐赠 ASF 进⾏独⽴孵化。</p><p>2023 年 3 ⽉ 12 ⽇，Flink Table Store 项⽬顺利通过投票，正式进⼊ Apache 软件基⾦会 (ASF) 的孵化器，改名为 Apache Paimon (incubating)。</p><p>进⼊孵化器后，Paimon 得到了众多的关注，包括 阿⾥云、字节跳动、Bilibili、汽⻋之家、蚂蚁 等多家公司参与到 Apache Paimon 的贡献，也得到了⼴⼤⽤户的使⽤。</p><p>Paimon 和 Flink 集成也在继续，Paimon 集成了 Flink CDC，提出了全⾃动的 数据 + Schema 的同步，整库同步，带来更⾼性能的⼊湖、更低的⼊湖成本、更⽅便的⼊湖体验。</p><h3 id="1-架构"><a href="#1-架构" class="headerlink" title="1 架构"></a>1 架构</h3><p><img src="/img/paimon/paimon-1.png"></p><p>如上架构所示：</p><p><strong>读&#x2F;写</strong>： Paimon 支持多种读&#x2F;写数据和执行 OLAP 查询的方式。</p><ul><li>对于读取，它支持消费数据 <ul><li>从历史快照（批处理模式），</li><li>从最新的偏移量（在流模式下），或</li><li>以混合方式读取增量快照。</li></ul></li><li>对于写入，它支持来自数据库变更日志（CDC）的流式同步或来自离线数据的批量插入&#x2F;覆盖。</li></ul><p><strong>生态系统</strong>：除了Apache Flink之外，Paimon还支持Apache Hive、Apache Spark、Trino等其他计算引擎的读取。</p><p><strong>内部</strong>：在底层，Paimon 将列式文件存储在文件系统&#x2F;对象存储上，并使用 LSM 树结构来支持大量数据更新和高性能查询。</p><h3 id="2-统一存储"><a href="#2-统一存储" class="headerlink" title="2 统一存储"></a>2 统一存储</h3><p>对于 Apache Flink 这样的流引擎，通常有三种类型的连接器：</p><ul><li>消息队列，例如 Apache Kafka，在该管道的源阶段和中间阶段都使用它，以保证延迟保持在秒级。</li><li>OLAP系统，例如ClickHouse，它以流方式接收处理后的数据并服务于用户的即席查询。</li><li>批量存储，例如Apache Hive，它支持传统批处理的各种操作，包括INSERT OVERWRITE.</li></ul><p>Paimon 提供表抽象。它的使用方式与传统数据库没有什么区别：</p><ul><li>在batch执行模式下，它就像一个Hive表，支持Batch SQL的各种操作。查询它以查看最新的快照。</li><li>在streaming执行模式下，它的作用就像一个消息队列。查询它的行为就像从历史数据永不过期的消息队列中查询流更改日志。</li></ul><h3 id="3-Streaming-Lakehouse-架构："><a href="#3-Streaming-Lakehouse-架构：" class="headerlink" title="3 Streaming Lakehouse 架构："></a>3 Streaming Lakehouse 架构：</h3><p>1、数据全链路实时流动，同时沉淀所有数据，提供 AD-HOC 查询；</p><p>2、通⽤的离线数据实时化，流批融合的⼀套数仓。</p><p><img src="/img/paimon/paimon-3.png"></p>]]></content>
    
    
    <categories>
      
      <category>Apache Paimon</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Apache Paimon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hbase -- 数据模型</title>
    <link href="/2023/10/09/bigDataComponents/HBase%20--%20%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/"/>
    <url>/2023/10/09/bigDataComponents/HBase%20--%20%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>Hbase 是一个面向<strong>列式存储</strong>的分布式数据库，其设计思想来源于 Google 的 BigTable 论文。Hbase 底层存储基于 HDFS 实现，集群的管理基于 ZooKeeper 实现。</p><p>Hbase 良好的分布式架构设计为海量数据的快速存储、随机访问提供了可能，基于<strong>数据副本机制</strong>和<strong>分区机制</strong>可以轻松实现在线扩容、缩容和数据容灾，是大数据领域中 Key - Value 数据结构存储最常用的数据库方案。</p><h2 id="1-特点"><a href="#1-特点" class="headerlink" title="1 特点"></a>1 特点</h2><h3 id="易扩展"><a href="#易扩展" class="headerlink" title="易扩展"></a>易扩展</h3><p>Hbase 的扩展性体现在两个方面：一是基于运算能力（RegionServer）的扩展，通过增加 RegionServer 节点的数量，提升 Hbase 上层的处理能力；另一个是基于存储能力的扩展（HDFS）通过增加 DataNode 节点数量对存储层的进行扩容，提升 HBase 的数据存储能力。</p><h3 id="海量存储"><a href="#海量存储" class="headerlink" title="海量存储"></a>海量存储</h3><p>HBase 作为一个开源的分布式 Key-Value 数据库，其主要作用是面向 PB 级别数据的实时入库和快速随机访问。这主要源于上述易扩展的特点，使得 HBase 通过扩展来存储海量的数据。</p><h3 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h3><p>Hbase 是根据列族来存储数据的。列族下面可以有非常多的列。列式存储的最大好处就是，其数据在表中是按照某列存储的，这样在查询只需要少数几个字段时，能大大减少读取的数据量。</p><h3 id="高可靠性"><a href="#高可靠性" class="headerlink" title="高可靠性"></a>高可靠性</h3><p>WAL 机制保证了数据写入时不会因集群异常而导致写入数据丢失，Replication 机制保证了在集群出现严重的问题时，数据不会发生丢失或损坏。而且 Hbase 底层使用 HDFS，HDFS 本身也有备份。</p><h3 id="稀疏性"><a href="#稀疏性" class="headerlink" title="稀疏性"></a>稀疏性</h3><p>在 HBase 的列族中，可以指定任意多的列，为空的列不占用存储空间，表可以设计得非常稀疏。</p><h2 id="2-模块组成"><a href="#2-模块组成" class="headerlink" title="2 模块组成"></a>2 模块组成</h2><p>HBase 可以将数据存储在本地文件系统，也可以存储在 HDFS 文件系统。在生产环境中，HBase 一般运行在 HDFS 上，以 HDFS 作为基础的存储设施。HBase 通过 HBase Client 提供的 Java API 来访问 HBase 数据库，以完成数据的写入和读取。HBase 集群主由HMaster、Region Server 和 ZooKeeper 组成。<br><img src="/img/bigDataComponents/hbase/hbase-architecture-1.png"></p><h3 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h3><ul><li>负责管理 RegionServer，实现其负载均衡；</li><li>管理和分配 Region，比如在 Region split时分配新的 Region，在 RegionServer 退出时迁移其内的 Region 到其他 RegionServer上；</li><li>管理namespace和table的元数据（实际存储在HDFS上）；</li><li>权限控制（ACL）。</li></ul><h3 id="RegionServer"><a href="#RegionServer" class="headerlink" title="RegionServer"></a>RegionServer</h3><ul><li>存放和管理本地 Region；</li><li>读写HDFS，管理Table中的数据；</li><li>Client 从 HMaster 中获取元数据，找到 RowKey 所在的 RegionServer 进行读写数据。</li></ul><h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3><ul><li>存放整个 HBase集群的元数据以及集群的状态信息；</li><li>实现HMaster主从节点的failover。</li></ul><h2 id="3-数据模型"><a href="#3-数据模型" class="headerlink" title="3 数据模型"></a>3 数据模型</h2><p>前面我们有讲过 HBase 是一个面向列式存储的分布式数据库，它的数据模型与 BigTable 十分相似。在 HBase 表中，一条数据拥有一个全局唯一的键(RowKey)和任意数量的列(Column)，一列或多列组成一个列族(Column Family)，同一个列族中列的数据在物理上都存储在同一个 HFile 中，这样基于列存储的数据结构有利于数据缓存和查询。</p><p>同时，HBase 会将表按主键划分为多个 Region 存储在不同 Region Server 上，以完成数据的分布式存储和读取。<br><img src="/img/bigDataComponents/hbase/hbase-architecture-2.png"></p><h3 id="Column-Family"><a href="#Column-Family" class="headerlink" title="Column Family"></a>Column Family</h3><p>一般同一类的列会放在一个列族中，每个列族都有一组存储属性：</p><ul><li>是否应该缓存在内存中；</li><li>数据如何被压缩或行键如何编码等。</li></ul><p>HBase 在创建表的时候就必须指定列族。HBase 的列族不是越多越好，官方推荐一个表的列族数量最好小于或者等于3，过多的列族不利于 HBase 数据的管理和索引。</p><h3 id="RowKey"><a href="#RowKey" class="headerlink" title="RowKey"></a>RowKey</h3><p>RowKey 的概念与关系型数据库中的主键相似，HBase 使用 RowKey 来唯一标识某行的数据。</p><p>访问 HBase 数据的方式有三种：</p><ul><li>基于 RowKey的单行查询；</li><li>基于RowKey的范围查询；</li><li>全表扫描查询。</li></ul><h3 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h3><p>HBase 按照行的方向将表中的数据基于 RowKey 的不同范围划分到不同 Region 上，每个Region都负责一定范围的数据存储和访问。<br><img src="/img/bigDataComponents/hbase/hbase-3.png"></p><p>每个表一开始只有一个 Region，随着数据不断插入表，Region 不断增大，当增大到一个阀值的时候，Region 就会等分成两个新的 Region。当table中的行不断增多，就会有越来越多的 Region。</p><p>另外，Region 是 Hbase 中分布式存储和负载均衡的最小单元，不同的 Region 可以分布在不同的 Region Server上。但一个 Region 是不会拆分到多个 Server 上的。</p><p>这样即使有一个包括上百亿条数据的表，由于数据被划分到不同的 Region上，每个 Region 都可以独立地进行写入和查询，HBase 写入或者查询的时候可以基于多 Region 分布式并发操作，因此访问速度也不会有太大的降低。</p><h3 id="TimeStamp"><a href="#TimeStamp" class="headerlink" title="TimeStamp"></a>TimeStamp</h3><p>TimeStamp 是实现 HBase 多版本的关键。在HBase 中，使用不同 TimeStamp 来标识相同RowKey对应的不同版本的数据。相同 RowKey的数据按照 TimeStamp 倒序排列。默认查询的是最新的版本，当然用户也可以指定 TimeStamp 的值来读取指定版本的数据。</p>]]></content>
    
    
    <categories>
      
      <category>Hbase</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hbase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HBase -- 是如何写入数据的</title>
    <link href="/2023/10/09/bigDataComponents/HBase%20--%20%E6%98%AF%E5%A6%82%E4%BD%95%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84/"/>
    <url>/2023/10/09/bigDataComponents/HBase%20--%20%E6%98%AF%E5%A6%82%E4%BD%95%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/bigDataComponents/hbase/hbase-6.png"></p><h2 id="Region-Server-寻址"><a href="#Region-Server-寻址" class="headerlink" title="Region Server 寻址"></a>Region Server 寻址</h2><ol><li>HBase Client 访问 ZooKeeper；</li><li>获取写入 Region 所在的位置，即获取 hbase:meta 表位于哪个 Region Server；</li><li>访问对应的 Region Server；</li><li>获取 hbase:meta 表，并查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 Region 信息以及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问；</li></ol><h2 id="写-Hlog"><a href="#写-Hlog" class="headerlink" title="写 Hlog"></a>写 Hlog</h2><ol start="5"><li>HBase Client 向 Region Server 发送写 Hlog 请求；</li><li>Region Server 会通过顺序写入磁盘的方式，将 Hlog 存储在 HDFS 上；</li></ol><h2 id="写-MemStore-并返回结果"><a href="#写-MemStore-并返回结果" class="headerlink" title="写 MemStore 并返回结果"></a>写 MemStore 并返回结果</h2><ol start="7"><li>HBase Client 向 Region Server 发送写 MemStore 请求；</li><li></li></ol>]]></content>
    
    
    <categories>
      
      <category>HBase</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HBase -- 架构组成</title>
    <link href="/2023/10/09/bigDataComponents/HBase%20--%20%E6%9E%B6%E6%9E%84%E7%BB%84%E6%88%90/"/>
    <url>/2023/10/09/bigDataComponents/HBase%20--%20%E6%9E%B6%E6%9E%84%E7%BB%84%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p>HBase 的核心架构由五部分组成，分别是 <code>HBase Client</code>、<code>HMaster</code>、<code>Region Server</code>、<code>ZooKeeper</code> 以及 <code>HDFS</code>。它的架构组成如下图所示。<br><img src="/img/bigDataComponents/hbase/hbase-4.png"></p><p>下面我们对 HBase 架构组成的每一部分详细介绍一下。</p><h2 id="HBase-Client"><a href="#HBase-Client" class="headerlink" title="HBase Client"></a>HBase Client</h2><p>HBase Client 为用户提供了访问 HBase 的接口，可以通过元数据表来定位到目标数据的 RegionServer，另外 HBase Client 还维护了对应的 cache 来加速 Hbase 的访问，比如缓存元数据的信息。</p><h2 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h2><p>HMaster 是 HBase 集群的主节点，负责整个集群的管理工作，主要工作职责如下：</p><ul><li><p>分配Region：负责启动的时候分配Region到具体的 RegionServer；</p></li><li><p>负载均衡：一方面负责将用户的数据均衡地分布在各个 Region Server 上，防止Region Server数据倾斜过载。另一方面负责将用户的请求均衡地分布在各个 Region Server 上，防止Region Server 请求过热；</p></li><li><p>维护数据：发现失效的 Region，并将失效的 Region 分配到正常的 RegionServer 上，并且在Region Sever 失效的时候，协调对应的HLog进行任务的拆分。</p></li></ul><h2 id="Region-Server"><a href="#Region-Server" class="headerlink" title="Region Server"></a>Region Server</h2><p>Region Server 直接对接用户的读写请求，是真正的干活的节点，主要工作职责如下。</p><ul><li><p>管理 HMaster 为其分配的 Region；</p></li><li><p>负责与底层的 HDFS 交互，存储数据到 HDFS；</p></li><li><p>负责 Region 变大以后的拆分以及 StoreFile 的合并工作。</p></li></ul><p>与 HMaster 的协同：当某个 RegionServer 宕机之后，ZK 会通知 Master 进行失效备援。下线的 RegionServer 所负责的 Region 暂时停止对外提供服务，Master 会将该 RegionServer 所负责的 Region 转移到其他 RegionServer 上，并且会对所下线的 RegionServer 上存在 MemStore 中还未持久化到磁盘中的数据由 Hlog 重播进行恢复。</p><p>下面给大家详细介绍下 Region Serve数据存储的基本结构，如下图所示。一个 Region Server 是包含多个 Region 的，这里仅展示一个。<br><img src="/img/bigDataComponents/hbase/hbase-5.png"></p><ul><li><p>Region：每一个 Region 都有起始 RowKey 和结束 RowKey，代表了存储的Row的范围，保存着表中某段连续的数据。一开始每个表都只有一个 Region，随着数据量不断增加，当 Region 大小达到一个阀值时，Region 就会被 Regio Server 水平切分成两个新的 Region。当 Region 很多时，HMaster 会将 Region 保存到其他 Region Server 上。</p></li><li><p>Store：一个 Region 由多个 Store 组成，每个 Store 都对应一个 Column Family, Store 包含 MemStore 和 StoreFile。</p><ul><li>MemStore：作为HBase的内存数据存储，数据的写操作会先写到 MemStore 中，当MemStore 中的数据增长到一个阈值（默认64M）后，Region Server 会启动 flasheatch 进程将 MemStore 中的数据写入 StoreFile 持久化存储，每次写入后都形成一个单独的 StoreFile。当客户端检索数据时，先在 MemStore中查找，如果MemStore 中不存在，则会在 StoreFile 中继续查找。</li><li>StoreFile：MemStore 内存中的数据写到文件后就是StoreFile，StoreFile底层是以 HFile 的格式保存。HBase以Store的大小来判断是否需要切分Region。当一个Region 中所有 StoreFile 的大小和数量都增长到超过一个阈值时，HMaster 会把当前Region分割为两个，并分配到其他 Region Server 上，实现负载均衡。</li></ul></li><li><p>HFile：HFile 和 StoreFile 是同一个文件，只不过站在 HDFS 的角度称这个文件为HFile，站在HBase的角度就称这个文件为StoreFile。</p></li><li><p>HLog：负责记录着数据的操作日志，当HBase出现故障时可以进行日志重放、故障恢复。例如，磁盘掉电导致 MemStore中的数据没有持久化存储到 StoreFile，这时就可以通过HLog日志重放来恢复数据。</p></li></ul><h2 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h2><p>HBase 通过 ZooKeeper 来完成选举 HMaster、监控 Region Server、维护元数据集群配置等工作，主要工作职责如下：</p><ul><li><p>选举HMaster：通ooKeeper来保证集中有1HMaster在运行，如果 HMaster 异常，则会通过选举机制产生新的 HMaster 来提供服务；</p></li><li><p>监控Region Server: 通过 ZooKeeper 来监控 Region Server 的状态，当Region Server 有异常的时候，通过回调的形式通知 HMaster 有关Region Server 上下线的信息；</p></li><li><p>维护元数据和集群配置：通过ZooKeeper储B信息并对外提供访问接口。</p></li></ul><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>HDFS 为 HBase 提供底层数据存储服务，同时为 HBase提供高可用的支持， HBase 将 HLog 存储在 HDFS 上，当服务器发生异常宕机时，可以重放 HLog 来恢复数据。</p>]]></content>
    
    
    <categories>
      
      <category>HBase</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JAVA 基础 -- CompletableFuture + 自定义线程池 实现多线程并发</title>
    <link href="/2023/10/07/java/JAVA%20%E5%9F%BA%E7%A1%80%20--%20CompletableFuture%20+%20%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BA%BF%E7%A8%8B%E6%B1%A0%20%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%B9%B6%E5%8F%91/"/>
    <url>/2023/10/07/java/JAVA%20%E5%9F%BA%E7%A1%80%20--%20CompletableFuture%20+%20%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BA%BF%E7%A8%8B%E6%B1%A0%20%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%B9%B6%E5%8F%91/</url>
    
    <content type="html"><![CDATA[<h2 id="1-背景-需求"><a href="#1-背景-需求" class="headerlink" title="1 背景 &amp; 需求"></a>1 背景 &amp; 需求</h2><p>最近工作中遇到了一个场景：需要将多个 zip 包中的文件解压之后上传到 hdfs，串行解压上传的速度非常慢；需要改造成并行执行。</p><h2 id="2-实现：CompletableFuture-自定义线程池"><a href="#2-实现：CompletableFuture-自定义线程池" class="headerlink" title="2 实现：CompletableFuture + 自定义线程池"></a>2 实现：CompletableFuture + 自定义线程池</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-meta">@Component</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">DataCheck</span> &#123;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 获取当前机器的核心数</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">AVAILABLE_PROCESSORS</span> <span class="hljs-operator">=</span> Runtime.getRuntime().availableProcessors();<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">ThreadPoolExecutor</span> <span class="hljs-variable">THREAD_POOL_EXECUTOR</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ThreadPoolExecutor</span>(<br>            <span class="hljs-number">2</span> * AVAILABLE_PROCESSORS,<br>            <span class="hljs-number">4</span> * AVAILABLE_PROCESSORS,<br>            <span class="hljs-number">3</span>, TimeUnit.SECONDS,<br>            <span class="hljs-keyword">new</span> <span class="hljs-title class_">LinkedBlockingDeque</span>&lt;&gt;(<span class="hljs-number">1024</span>),<br>            Executors.defaultThreadFactory(),<br>            <span class="hljs-keyword">new</span> <span class="hljs-title class_">ThreadPoolExecutor</span>.AbortPolicy());<br><br>    <span class="hljs-meta">@Scheduled(cron = &quot;0 */1 * * * ?&quot;)</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">upload</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1 获取 check_status = 1 的目录</span><br>        List&lt;FileCheckLog&gt; fileCheckLogs = xxx;<br>        List&lt;CompletableFuture&lt;Void&gt;&gt; futures = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        <span class="hljs-keyword">for</span> (FileCheckLog fileCheckLog : fileCheckLogs) &#123;<br>            CompletableFuture&lt;Void&gt; future = CompletableFuture.runAsync(() -&gt; &#123;<br>                <span class="hljs-comment">// ... ...</span><br>            &#125;, THREAD_POOL_EXECUTOR);<br>            futures.add(future);<br>        &#125;<br>        CompletableFuture.allOf(futures.toArray(<span class="hljs-keyword">new</span> <span class="hljs-title class_">CompletableFuture</span>[<span class="hljs-number">0</span>])).join();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>JAVA 基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JAVA 基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flink 基础学习 -- 如何保障实时指标的质量</title>
    <link href="/2023/09/27/flink/Flink%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%20--%20%E5%A6%82%E4%BD%95%E4%BF%9D%E9%9A%9C%E5%AE%9E%E6%97%B6%E6%8C%87%E6%A0%87%E7%9A%84%E8%B4%A8%E9%87%8F/"/>
    <url>/2023/09/27/flink/Flink%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%20--%20%E5%A6%82%E4%BD%95%E4%BF%9D%E9%9A%9C%E5%AE%9E%E6%97%B6%E6%8C%87%E6%A0%87%E7%9A%84%E8%B4%A8%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<p>可以从事前、事中、事后三个时间的任务层面、指标层面去看：</p><h2 id="1-事前"><a href="#1-事前" class="headerlink" title="1 事前"></a>1 事前</h2><ul><li>任务层面：根据峰值流量进行压力测试，并且保留一定buffer，保障任务在资源层面没有瓶颈；</li><li>指标层面：根据业务需求，上线实时任务前，在相同的口径下对实时指标和离线指标进行对比，在实时指标的误差不超过业务阈值时，才能上线。</li></ul><h2 id="2-事中"><a href="#2-事中" class="headerlink" title="2 事中"></a>2 事中</h2><ul><li>任务层面：贴源层监控 Kafka 堆积延迟等报警检测手段，用于事中及时发现问题。比如：用 Promethues 监控 Lag 时长；</li><li>指标层面：定期将实时结果导到离线，然后与离线指标对比，超过阈值则告警。</li></ul><h2 id="3-事后"><a href="#3-事后" class="headerlink" title="3 事后"></a>3 事后</h2><ul><li>任务层面：对于可能发生的故障类型，构建用于故障恢复、数据回溯的实时任务备用链路，比如：实时任务出现问题之后，可以将备用的离线链路顶上；</li><li>指标层面：构建指标修复预案，根据不同的故障类型，判断是否可以使⽤实时任务进⾏修复；如果实时⽆法修复，构建离线恢复链路，以便使⽤离线数据进⾏覆写修复。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Flink 基础学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flink 基础学习 -- 自定义 Connector</title>
    <link href="/2023/09/27/flink/Flink%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%20--%20%E8%87%AA%E5%AE%9A%E4%B9%89%20Connector/"/>
    <url>/2023/09/27/flink/Flink%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%20--%20%E8%87%AA%E5%AE%9A%E4%B9%89%20Connector/</url>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h2><p>在许多情况下，开发者不需要从头开始创建新的连接器，而是希望稍微修改现有的连接器或挂钩到现有的堆栈。在其他情况下，实施者希望创建专门的连接器。</p><p>本节对这两种用例都有帮助。它解释了表连接器的一般架构，从 API 中的纯声明到将在集群上执行的运行时代码。</p><p>实心箭头显示在转换过程中对象如何从一个阶段转换为其他对象。<br><img src="/img/flink/flink-table-connectors.svg"></p><h2 id="2-MataData"><a href="#2-MataData" class="headerlink" title="2 MataData"></a>2 MataData</h2><p>Table API 和 SQL 都是声明式 API。这包括表的声明。因此，执行 CREATE TABLE 语句会导致目标目录中的元数据更新。</p><p>对于大多数目录实现，不会针对此类操作修改外部系统中的物理数据。连接器特定的依赖项不必出现在类路径中。在WITH子句中声明的选项既不经过验证也不以其他方式解释。</p><p>动态表的元数据（通过 DDL 创建或由目录提供）表示为 CatalogTable 的实例。必要时，表名将在内部解析为 CatalogTable。</p><h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p>在规划和优化表程序时，需要将 CatalogTable 解析为 DynamicTableSource（用于在 SELECT 查询中读取）和 DynamicTableSink（用于在 INSERT INTO 语句中写入）。</p><p>DynamicTableSourceFactory 和 DynamicTableSinkFactory 提供特定于连接器的逻辑，用于将 CatalogTable 的元数据转换为 DynamicTableSource 和 DynamicTableSink 的实例。在大多数情况下，工厂的目的是验证选项（例如示例中的“port”&#x3D;“5022”）、配置编码&#x2F;解码格式（如果需要）以及创建表连接器的参数化实例。</p><p>默认情况下，DynamicTableSourceFactory 和 DynamicTableSinkFactory 的实例是使用 Java 的服务提供者接口 (SPI) 发现的。连接器选项（例如示例中的“connector”&#x3D;“custom”）必须对应于有效的工厂标识符。</p><p>尽管在类命名中可能并不明显，但 DynamicTableSource 和 DynamicTableSink 也可以被视为有状态工厂，它们最终生成用于读取&#x2F;写入实际数据的具体运行时实现。</p><p>规划器使用源实例和接收器实例执行特定于连接器的双向通信，直到找到最佳逻辑计划。根据可选声明的能力接口（例如 SupportsProjectionPushDown 或 SupportsOverwrite），规划器可能会对实例应用更改，从而改变生成的运行时实现。</p><h2 id="Runtime"><a href="#Runtime" class="headerlink" title="Runtime"></a>Runtime</h2><p>一旦逻辑规划完成，规划器将从表连接器获取运行时实现。运行时逻辑在 Flink 的核心连接器接口（例如 InputFormat 或 SourceFunction）中实现。</p><p>这些接口按另一个抽象级别分组为 ScanRuntimeProvider、LookupRuntimeProvider 和 SinkRuntimeProvider 的子类。</p><p>例如，OutputFormatProvider（提供 org.apache.flink.api.common.io.OutputFormat）和 SinkFunctionProvider（提供 org.apache.flink.streaming.api.functions.sink.SinkFunction）都是规划者可以使用的 SinkRuntimeProvider 的具体实例处理。</p><p>链接：</p><ol><li><a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/dev/table/sourcessinks/">Flink User-defined Sources &amp; Sinks</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>Flink 基础学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flink 错误排查 -- Flink SQL 统计 CDC 数据源结果错误</title>
    <link href="/2023/09/27/flink/Flink%20%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%20--%20Flink%20SQL%20%E5%AF%B9%20CDC%20%E6%95%B0%E6%8D%AE%E6%BA%90%E5%81%9A%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C%E9%94%99%E8%AF%AF/"/>
    <url>/2023/09/27/flink/Flink%20%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%20--%20Flink%20SQL%20%E5%AF%B9%20CDC%20%E6%95%B0%E6%8D%AE%E6%BA%90%E5%81%9A%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C%E9%94%99%E8%AF%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p>在对 CDC 数据源的数值类型做统计操作（SUM、COUNT）时，发现结果与预期结果不一致。</p><h2 id="2-问题原因与解决方案"><a href="#2-问题原因与解决方案" class="headerlink" title="2 问题原因与解决方案"></a>2 问题原因与解决方案</h2><p>将参数 <strong>table.exec.source.cdc-events-duplicate</strong> 设置为 <strong>true</strong>。</p><p>原因：</p><p>表示作业中的CDC（变更数据捕获）源是否会产生重复的变更事件，这些事件要求框架消除重复并获得一致的结果。CDC源是指生成完整更改事件的源，</p><p>包括INSERT&#x2F;UPDATE_BEFORE&#x2F;UPDATE_AFTER&#x2F;DELETE，例如Debezium格式的Kafka源。默认情况下，此配置的值为false。 </p><p>但是，常见的情况是存在重复的更改事件。因为通常情况下，当发生故障切换时，CDC工具（例如Debezium）至少在一次交付中工作。因此，在异常情况下，Debezium可能会向Kafka传递重复的更改事件，Flink将获得重复的事件。</p><p>这可能会导致Flink查询得到错误的结果或意外的异常。 因此，如果您的CDC工具至少交付一次，建议打开此配置。启用此配置需要在CDC源上定义PRIMARY KEY。主键将用于消除重复的更改事件，并生成标准化的更改日志流，但需要额外的有状态运算符。</p><p>会生成一个 ChangelogNormalize 状态算子来生成完整的 changelog：</p><p><img src="/img/flink/flink-cdc-events-duplicate.png"><br><img src="/img/flink/flink-cdc-events-duplicate-1.png"></p>]]></content>
    
    
    <categories>
      
      <category>Flink 错误排查</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flink 错误排查 -- Flink metrics 指标断断续续 - Prometheus + PushGateway + Grafana</title>
    <link href="/2023/09/20/flink/Flink%20%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%20--%20Flink%20metrics%20%E6%8C%87%E6%A0%87%E6%96%AD%E6%96%AD%E7%BB%AD%E7%BB%AD/"/>
    <url>/2023/09/20/flink/Flink%20%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%20--%20Flink%20metrics%20%E6%8C%87%E6%A0%87%E6%96%AD%E6%96%AD%E7%BB%AD%E7%BB%AD/</url>
    
    <content type="html"><![CDATA[<p>Prometheus + PushGateway + Grafana 是一套常用的监控 Flink metrics 的组合。网上有很多实践，这里就不赘述了。但是在使用的过程中还是遇到了一下问题。</p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>如果一个任务有多个 taskmanager ，dashboard 上 taskmanager 相关的指标则是断断续续的，如图所示：<br><img src="/img/flink/flink-taskmanager-dashboard.png"></p><p>导致 metrics 断断续续的另外一个原因：<a href="https://blog.csdn.net/daijiguo/article/details/105453643">Flink问题：记Flink Metrics时断时续问题排查</a>；不过我已经将 push 方法改成 pushAdd 方法之后重新打包。</p><p>当前 PushGateway 相关的一些配置：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql">metrics.reporter.promgateway.factory.class: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporterFactory<br>metrics.reporter.promgateway.hostUrl: http:<span class="hljs-operator">/</span><span class="hljs-operator">/</span>xxxx:<span class="hljs-number">9091</span><br>metrics.reporter.promgateway.randomJobNameSuffix: <span class="hljs-literal">false</span><br>metrics.reporter.promgateway.deleteOnShutdown: <span class="hljs-literal">true</span><br>metrics.reporter.promgateway.interval: <span class="hljs-number">30</span>s<br>metrics.reporter.promgateway.groupingKey: type<span class="hljs-operator">=</span>flink;env<span class="hljs-operator">=</span>test<br>metrics.reporter.promgateway.jobName: xxx <span class="hljs-comment">-- 提交任务的时候动态配置</span><br></code></pre></td></tr></table></figure><h2 id="1-问题原因"><a href="#1-问题原因" class="headerlink" title="1 问题原因"></a>1 问题原因</h2><p>PushGateway 是按照 jobName、groupingKey 和 metric_name 更新数据的，测试：<br><img src="/img/flink/flink-metrics-1.png"><br><img src="/img/flink/flink-metrics-2.png"><br>可以发现在 jobName &#x3D; my_job、groupingKey &#x3D; type&#x3D;flink;env&#x3D;test、metric_name &#x3D; my_custom_metrics 的情况下，上报了两次指标，<br>第一次为 tm_id&#x3D;”container_e64_1691562725918_0295_01_000001” 的指标；<br>第二次为 tm_id&#x3D;”container_e64_1691562725918_0295_01_000002” 的指标；<br>但是第二次的指标直接把第一次的覆盖了，所以在多个 taskmanager 的情况下，同一个指标会相互覆盖，出现指标断断续续的情况。</p><h2 id="2-解决办法"><a href="#2-解决办法" class="headerlink" title="2 解决办法"></a>2 解决办法</h2><p>将 <strong>metrics.reporter.promgateway.randomJobNameSuffix</strong> 设置为 true，这样每个 container（jobmanager、taskmanager） 在 jobName 之后会加上一个随机后缀（如下图所示），<br>同时在 <strong>metrics.reporter.promgateway.groupingKey</strong> 在加上一个 <strong>job_name&#x3D;xxx</strong> 的 key，可以看到 PushGateway 上每个 container 都会生成一个 group，不会再出现不同 taskmanager 的同一个 metric 相互覆盖的情况。<br><img src="/img/flink/flink-metrics-3.png"></p><p>在 Grafana Dashboard 中我们可以通过 groupingKey 中加上的 job_name&#x3D;xxx 进行分类筛选：<br><img src="/img/flink/flink-metrics-4.png"><br><img src="/img/flink/flink-metrics-5.png"></p>]]></content>
    
    
    <categories>
      
      <category>Flink 错误排查</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flink 基础学习 -- 如何向 UDF 传递参数</title>
    <link href="/2023/08/22/flink/Flink%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%20--%20%E5%A6%82%E4%BD%95%E5%90%91%20UDF%20%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0/"/>
    <url>/2023/08/22/flink/Flink%20%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%20--%20%E5%A6%82%E4%BD%95%E5%90%91%20UDF%20%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>SQL API 中加上：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SET</span> <span class="hljs-string">&#x27;pipeline.global-job-parameters&#x27;</span><span class="hljs-operator">=</span><span class="hljs-string">&#x27;env:prod&#x27;</span>;<br></code></pre></td></tr></table></figure><p>然后在 UserDefinedFunction 的 open 方法中获取：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">open</span><span class="hljs-params">(FunctionContext context)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>    String currentEnv=context.getJobParameter(ENV,Constant.TEST);<br>    ... ...<br>&#125;<br><br></code></pre></td></tr></table></figure><p>相关链接：<a href="https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/">https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/</a></p>]]></content>
    
    
    <categories>
      
      <category>Flink 基础学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flink 错误排查 -- Elasticsearch Sink - java.lang.OutOfMemoryError Direct buffer memory</title>
    <link href="/2023/08/22/flink/Flink%20%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%20--%20Elasticsearch%20Sink%EF%BC%9Ajava.lang.OutOfMemoryError:%20Direct%20buffer%20memory/"/>
    <url>/2023/08/22/flink/Flink%20%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%20--%20Elasticsearch%20Sink%EF%BC%9Ajava.lang.OutOfMemoryError:%20Direct%20buffer%20memory/</url>
    
    <content type="html"><![CDATA[<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1 问题描述"></a>1 问题描述</h2><p>背景：近期上线了一个写 es 的 flink 任务，但是任务一直重启，然后进到 taskmanager 的日志找到了报错信息：<br><img src="/img/flink/flink-es-sink.png"></p><h2 id="2-问题原因"><a href="#2-问题原因" class="headerlink" title="2 问题原因"></a>2 问题原因</h2><p>因为写 es 的时候会把数据缓存在 直接内存（Direct Memory）&#x2F; 堆外内存 中，由报错信息可以看出，由于数据量比较大导致直接内存 OutOfMemory。</p><h2 id="3-解决办法"><a href="#3-解决办法" class="headerlink" title="3 解决办法"></a>3 解决办法</h2><p><img src="/img/flink/flink-taskmanager.png"><br>由 taskmanager 的内存模型来看，Direct Memory 由三部分组成：Framework Off-Heap、Task Off-Heap、Network；这里主要是用到了 Framework Off-Heap 这部分内存，所以可以调大 Framework Off-Heap 的内存：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-operator">-</span>Dtaskmanager.memory.framework.off<span class="hljs-operator">-</span>heap.size<span class="hljs-operator">=</span><span class="hljs-number">640</span>m<br></code></pre></td></tr></table></figure><p>链接：</p><ol><li><a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/deployment/memory/mem_setup_tm/">Flink taskmanager 内存模型</a></li><li><a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/zh/docs/deployment/memory/mem_trouble/">Flink 内存常见问题</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>Flink 错误排查</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flink + StarRocks -- 用户行为挖掘与线索推荐</title>
    <link href="/2023/08/22/project/Flink%20+%20StarRocks%20--%20%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%8C%96%E6%8E%98%E4%B8%8E%E7%BA%BF%E7%B4%A2%E6%8E%A8%E8%8D%90/"/>
    <url>/2023/08/22/project/Flink%20+%20StarRocks%20--%20%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%8C%96%E6%8E%98%E4%B8%8E%E7%BA%BF%E7%B4%A2%E6%8E%A8%E8%8D%90/</url>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>用户在我们官网上或者是APP上会各种各样的操作行为，其中有几种典型的行为是值得做商机挖掘的，比如搜索并点击、浏览、加入购物车等等，希望大数据可以抓取过来，记载到这个客户名下，作为与客户沟通的信息谈资，同时做数据挖掘，自动推送线索到线上、线下团队，提高商机转化率。</p><h2 id="2-需求"><a href="#2-需求" class="headerlink" title="2 需求"></a>2 需求</h2><p>根据需求我们需要筛选出过去180天满足线索规则的用户，比如：过去180天内浏览该商品超过10次，则符合要求；规则是按照公司和行为维度触发的，比如：同一个公司下多个用户触发了同一个行为，多个用户触发行为的累计值达到了阈值，就会触发线索，向下游发生信息。</p><h2 id="3-分析"><a href="#3-分析" class="headerlink" title="3 分析"></a>3 分析</h2><p>目前每天的数量大概是 5000w 条，那么180天的数据量为90亿条，数据量非常大，但是目前会对数据进行筛选，只保留固定几种行为的用户、剔除无效的行为（比如未登录用户的行为）之后，每天的数据量大概为 1w 条数据。考虑到未来的扩展性，所以在设计方案的时候需要考虑到数据量非常大的情况。</p><h2 id="4-设计方案"><a href="#4-设计方案" class="headerlink" title="4 设计方案"></a>4 设计方案</h2><h3 id="方案一：Flink-State-TTL"><a href="#方案一：Flink-State-TTL" class="headerlink" title="方案一：Flink State + TTL"></a>方案一：Flink State + TTL</h3><p>存在的问题：状态太大，占用过多的内存资源；任务失败之后恢复困难。</p><h3 id="方案二：StarRocks-明细模型-物化视图-分区TTL"><a href="#方案二：StarRocks-明细模型-物化视图-分区TTL" class="headerlink" title="方案二：StarRocks 明细模型 + 物化视图 + 分区TTL"></a>方案二：StarRocks 明细模型 + 物化视图 + 分区TTL</h3><h4 id="StarRocks-明细模型：存储用户行为的明细："><a href="#StarRocks-明细模型：存储用户行为的明细：" class="headerlink" title="StarRocks 明细模型：存储用户行为的明细："></a>StarRocks 明细模型：存储用户行为的明细：</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> IF <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> dwd_pol_user_events (<br>    `credit_code` STRING <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `internal_event_code`   STRING <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `user_id` STRING <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `phone_number` STRING <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `<span class="hljs-type">date</span>`    <span class="hljs-type">DATE</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span><br>)<br>    DUPLICATE KEY(`credit_code`, `internal_event_code`, `user_id`)<br>    <span class="hljs-keyword">PARTITION</span> <span class="hljs-keyword">BY</span> <span class="hljs-keyword">RANGE</span>(`<span class="hljs-type">date</span>`) (<br>    <span class="hljs-keyword">START</span> (&quot;2023-01-20&quot;) <span class="hljs-keyword">END</span> (&quot;2023-08-08&quot;) <span class="hljs-keyword">EVERY</span> (<span class="hljs-type">INTERVAL</span> <span class="hljs-number">1</span> <span class="hljs-keyword">DAY</span>)<br>    )<br>    DISTRIBUTED <span class="hljs-keyword">BY</span> HASH(`credit_code`, `internal_event_code`) BUCKETS <span class="hljs-number">6</span><br>    PROPERTIES (<br>        &quot;replication_num&quot; <span class="hljs-operator">=</span> &quot;3&quot;,<br>        &quot;dynamic_partition.enable&quot; <span class="hljs-operator">=</span> &quot;true&quot;,<br>        &quot;dynamic_partition.time_unit&quot; <span class="hljs-operator">=</span> &quot;DAY&quot;,<br>        &quot;dynamic_partition.start&quot; <span class="hljs-operator">=</span> &quot;-179&quot;,<br>        &quot;dynamic_partition.end&quot; <span class="hljs-operator">=</span> &quot;1&quot;,<br>        &quot;dynamic_partition.prefix&quot; <span class="hljs-operator">=</span> &quot;p&quot;,<br>        &quot;dynamic_partition.buckets&quot; <span class="hljs-operator">=</span> &quot;6&quot;);<br></code></pre></td></tr></table></figure><h4 id="物化视图：对-180-天的数据按照-company-internal-event-code（标签）-维度建物化视图，提高查询速度；"><a href="#物化视图：对-180-天的数据按照-company-internal-event-code（标签）-维度建物化视图，提高查询速度；" class="headerlink" title="物化视图：对 180 天的数据按照 company + internal_event_code（标签） 维度建物化视图，提高查询速度；"></a>物化视图：对 180 天的数据按照 company + internal_event_code（标签） 维度建物化视图，提高查询速度；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> MATERIALIZED <span class="hljs-keyword">VIEW</span> dwd_pol_user_events_view <span class="hljs-keyword">AS</span> <br><span class="hljs-keyword">SELECT</span> `credit_code`, `internal_event_code`, <span class="hljs-built_in">COUNT</span>(`user_id`) <br><span class="hljs-keyword">FROM</span> dwd_pol_user_events <br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> `credit_code`, `internal_event_code`;<br></code></pre></td></tr></table></figure><h4 id="分区TTL：设置-180-天的-TTL，StarRocks-只会保存-180-天的数据。"><a href="#分区TTL：设置-180-天的-TTL，StarRocks-只会保存-180-天的数据。" class="headerlink" title="分区TTL：设置 180 天的 TTL，StarRocks 只会保存 180 天的数据。"></a>分区TTL：设置 180 天的 TTL，StarRocks 只会保存 180 天的数据。</h4><p>参考建表语句中动态分区相关的配置。</p><h3 id="方案二：StarRocks-聚合模型-物化视图-分区TTL"><a href="#方案二：StarRocks-聚合模型-物化视图-分区TTL" class="headerlink" title="方案二：StarRocks 聚合模型 + 物化视图 + 分区TTL"></a>方案二：StarRocks 聚合模型 + 物化视图 + 分区TTL</h3><h4 id="StarRocks-聚合模型：可以按照-date-company-internal-event-code（标签）提前聚合，大大减少数据量；"><a href="#StarRocks-聚合模型：可以按照-date-company-internal-event-code（标签）提前聚合，大大减少数据量；" class="headerlink" title="StarRocks 聚合模型：可以按照 date + company + internal_event_code（标签）提前聚合，大大减少数据量；"></a>StarRocks 聚合模型：可以按照 date + company + internal_event_code（标签）提前聚合，大大减少数据量；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> IF <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> dwd_pol_user_events_agg (<br>    `credit_code` STRING <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `internal_event_code`   STRING <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `user_id` STRING <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `phone_number` STRING <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `<span class="hljs-type">date</span>`    <span class="hljs-type">DATE</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span>,<br>    `pv` <span class="hljs-type">BIGINT</span> SUM <span class="hljs-keyword">DEFAULT</span> &quot;1&quot; COMMENT &quot;total page views&quot;<br>)<br>    AGGREGATE KEY(`credit_code`, `internal_event_code`, `user_id`, `phone_number`, `<span class="hljs-type">date</span>`)<br>    <span class="hljs-keyword">PARTITION</span> <span class="hljs-keyword">BY</span> <span class="hljs-keyword">RANGE</span>(`<span class="hljs-type">date</span>`) (<br>    <span class="hljs-keyword">START</span> (&quot;2023-01-20&quot;) <span class="hljs-keyword">END</span> (&quot;2023-08-08&quot;) <span class="hljs-keyword">EVERY</span> (<span class="hljs-type">INTERVAL</span> <span class="hljs-number">1</span> <span class="hljs-keyword">DAY</span>)<br>    )<br>    DISTRIBUTED <span class="hljs-keyword">BY</span> HASH(`credit_code`, `internal_event_code`) BUCKETS <span class="hljs-number">6</span><br>    PROPERTIES (<br>        &quot;replication_num&quot; <span class="hljs-operator">=</span> &quot;3&quot;,<br>        &quot;dynamic_partition.enable&quot; <span class="hljs-operator">=</span> &quot;true&quot;,<br>        &quot;dynamic_partition.time_unit&quot; <span class="hljs-operator">=</span> &quot;DAY&quot;,<br>        &quot;dynamic_partition.start&quot; <span class="hljs-operator">=</span> &quot;-179&quot;,<br>        &quot;dynamic_partition.end&quot; <span class="hljs-operator">=</span> &quot;1&quot;,<br>        &quot;dynamic_partition.prefix&quot; <span class="hljs-operator">=</span> &quot;p&quot;,<br>        &quot;dynamic_partition.buckets&quot; <span class="hljs-operator">=</span> &quot;6&quot;);<br></code></pre></td></tr></table></figure><h4 id="物化视图：对-180-天的数据按照-company-internal-event-code（标签）维度建物化视图，提高查询速度；"><a href="#物化视图：对-180-天的数据按照-company-internal-event-code（标签）维度建物化视图，提高查询速度；" class="headerlink" title="物化视图：对 180 天的数据按照 company + internal_event_code（标签）维度建物化视图，提高查询速度；"></a>物化视图：对 180 天的数据按照 company + internal_event_code（标签）维度建物化视图，提高查询速度；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> MATERIALIZED <span class="hljs-keyword">VIEW</span> dwd_pol_user_events_agg_view <span class="hljs-keyword">AS</span> <br><span class="hljs-keyword">SELECT</span> `credit_code`, `internal_event_code`, <span class="hljs-built_in">SUM</span>(`pv`) <br><span class="hljs-keyword">FROM</span> dwd_pol_user_events_agg <br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> `credit_code`, `internal_event_code`;<br></code></pre></td></tr></table></figure><h4 id="分区TTL：设置-180-天的-TTL，StarRocks-只会保存-180-天的数据。-1"><a href="#分区TTL：设置-180-天的-TTL，StarRocks-只会保存-180-天的数据。-1" class="headerlink" title="分区TTL：设置 180 天的 TTL，StarRocks 只会保存 180 天的数据。"></a>分区TTL：设置 180 天的 TTL，StarRocks 只会保存 180 天的数据。</h4><p>参考建表语句中动态分区相关的配置。</p><h2 id="5-数据链路"><a href="#5-数据链路" class="headerlink" title="5 数据链路"></a>5 数据链路</h2><p><img src="/img/project/user-behavior-clue.png"></p><h3 id="每个-Flink-任务的功能："><a href="#每个-Flink-任务的功能：" class="headerlink" title="每个 Flink 任务的功能："></a>每个 Flink 任务的功能：</h3><ul><li>①：对数据进行筛选，只保留产品定好的的几种行为、剔除无效的行为（比如未登录用户的行为）；</li><li>②：负责将数据写入 StarRocks 和 Kafka，这边大家可能会有疑问，这里为什么要写到 Kafka 中？后面会解答。</li><li>③：消费 Kafka 中的数据，并查询 StarRocks 统计过去 180 天 company 维度下所有用户触发了该行为的累计值，并查询 mysql 行为的阈值表，对比是否触发了阈值，如果触发了阈值，还需要查询行为的结果表，该 company 是否已经触发了线索，因为规定同一个 company 只能触发一次线索；</li><li>④：消费 Kafka 中的数据，写入 elasticsearch 中，供用户查询详细的信息。</li></ul><h3 id="mysql-阈值表结构"><a href="#mysql-阈值表结构" class="headerlink" title="mysql 阈值表结构"></a>mysql 阈值表结构</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> dim_pol_clue_event_threshold<br>(<br>    internal_event_code <span class="hljs-type">varchar</span>(<span class="hljs-number">256</span>)                        <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;内部事件编码&#x27;</span><br>        <span class="hljs-keyword">primary</span> key,<br>    internal_event_name <span class="hljs-type">varchar</span>(<span class="hljs-number">256</span>)                        <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;内部事件名称&#x27;</span>,<br>    threshold_num       <span class="hljs-type">bigint</span>                              <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span>,<br>    create_time         <span class="hljs-type">timestamp</span> <span class="hljs-keyword">default</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;创建时间&#x27;</span>,<br>    update_time         <span class="hljs-type">timestamp</span> <span class="hljs-keyword">default</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">update</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span> comment <span class="hljs-string">&#x27;更新时间&#x27;</span><br>)<br>    comment <span class="hljs-string">&#x27;触发线索的时间阈值表&#x27;</span>;<br></code></pre></td></tr></table></figure><h3 id="mysql-结果表结构"><a href="#mysql-结果表结构" class="headerlink" title="mysql 结果表结构"></a>mysql 结果表结构</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> dwd_pol_clue_trigger_info<br>(<br>    credit_code         <span class="hljs-type">varchar</span>(<span class="hljs-number">256</span>)                        <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;内部事件编码&#x27;</span><br>        <span class="hljs-keyword">primary</span> key,<br>    company_name        <span class="hljs-type">varchar</span>(<span class="hljs-number">256</span>)                        <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;企业名称&#x27;</span>,<br>    phone_numbers       text                                <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;用户电话号码&#x27;</span>,<br>    internal_event_code <span class="hljs-type">varchar</span>(<span class="hljs-number">256</span>)                        <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;内部事件code&#x27;</span>,<br>    internal_event_name <span class="hljs-type">varchar</span>(<span class="hljs-number">256</span>)                        <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;内部事件名称&#x27;</span>,<br>    event_trigger_num   <span class="hljs-type">bigint</span>                              <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;触发值&#x27;</span>,<br>    create_time         <span class="hljs-type">timestamp</span> <span class="hljs-keyword">default</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> comment <span class="hljs-string">&#x27;创建时间&#x27;</span>,<br>    update_time         <span class="hljs-type">timestamp</span> <span class="hljs-keyword">default</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">update</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span> comment <span class="hljs-string">&#x27;更新时间&#x27;</span><br>)<br>    comment <span class="hljs-string">&#x27;线索触发信息&#x27;</span>;<br></code></pre></td></tr></table></figure><h2 id="6-其他事项"><a href="#6-其他事项" class="headerlink" title="6 其他事项"></a>6 其他事项</h2><h3 id="第③个-Flink-任务中为什么要写-Kafka"><a href="#第③个-Flink-任务中为什么要写-Kafka" class="headerlink" title="第③个 Flink 任务中为什么要写 Kafka"></a>第③个 Flink 任务中为什么要写 Kafka</h3><p>因为下游的 Flink 任务需要根据 Kafka 中的数据触发查询 StarRocks 的行为，这样才能推动流的处理。</p><h3 id="StarRocks-压测"><a href="#StarRocks-压测" class="headerlink" title="StarRocks 压测"></a>StarRocks 压测</h3><p>这里需要对 StarRocks 进行压测，防止数据量太大对 StarRocks 造成太大的压力。这里使用 JMeter 进行压测。<br>测试环境 StarRocks 配置：3 FE、3 BE（16C 68G）；<br>表的分桶数分别为 3 和 24（按照 建议的 BE * CPU &#x2F; 2）；<br>结果：310qps。<br>能够满足当前的数据量。</p><h3 id="Flink-查询-StarRocks-UDF"><a href="#Flink-查询-StarRocks-UDF" class="headerlink" title="Flink 查询 StarRocks UDF"></a>Flink 查询 StarRocks UDF</h3><p>Flink 去查询 StarRocks 时使用的是 UDF，一共两个 UDF：</p><ul><li><p>获取 company 下某个行为的累计值：<br>公共类：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">StarRocksJdbcConnectionProvider</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Serializable</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">Logger</span> <span class="hljs-variable">LOGGER</span> <span class="hljs-operator">=</span> LogManager.getLogger(StarRocksJdbcConnectionProvider.class);<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> <span class="hljs-variable">serialVersionUID</span> <span class="hljs-operator">=</span> <span class="hljs-number">1L</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> String env;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> <span class="hljs-keyword">volatile</span> Connection connection;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">StarRocksJdbcConnectionProvider</span><span class="hljs-params">(String env)</span> &#123;<br>        <span class="hljs-built_in">this</span>.env = env;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> Connection <span class="hljs-title function_">getConnection</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> SQLException, ClassNotFoundException &#123;<br>        <span class="hljs-keyword">if</span> (connection == <span class="hljs-literal">null</span>) &#123;<br>            <span class="hljs-keyword">synchronized</span> (<span class="hljs-built_in">this</span>) &#123;<br>                <span class="hljs-keyword">if</span> (connection == <span class="hljs-literal">null</span>) &#123;<br>                    Class.forName(StarRocksJdbcConnConfig.DRIVER_NAME);<br>                    <span class="hljs-keyword">if</span> (PROD.equalsIgnoreCase(env)) &#123;<br>                        connection = DriverManager.getConnection(<br>                                StarRocksJdbcConnConfig.PROD_URL,<br>                                StarRocksJdbcConnConfig.PROD_USERNAME,<br>                                StarRocksJdbcConnConfig.PROD_PASSWORD);<br>                    &#125; <span class="hljs-keyword">else</span> &#123;<br>                        connection = DriverManager.getConnection(<br>                                StarRocksJdbcConnConfig.TEST_URL,<br>                                StarRocksJdbcConnConfig.TEST_USERNAME,<br>                                StarRocksJdbcConnConfig.TEST_PASSWORD);<br>                    &#125;<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> connection;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> Connection <span class="hljs-title function_">reestablishConnection</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> SQLException, ClassNotFoundException &#123;<br>        close();<br>        connection = getConnection();<br>        <span class="hljs-keyword">return</span> connection;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">isConnectionValid</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> SQLException &#123;<br>        <span class="hljs-keyword">return</span> connection != <span class="hljs-literal">null</span><br>                &amp;&amp; connection.isValid(CONNECTION_CHECK_TIMEOUT_SECONDS);<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">if</span> (connection == <span class="hljs-literal">null</span>) &#123;<br>            <span class="hljs-keyword">return</span>;<br>        &#125;<br>        <span class="hljs-keyword">try</span> &#123;<br>            connection.close();<br>        &#125; <span class="hljs-keyword">catch</span> (SQLException e) &#123;<br>            LOGGER.error(<span class="hljs-string">&quot;JDBC connection close failed.&quot;</span>, e);<br>        &#125; <span class="hljs-keyword">finally</span> &#123;<br>            connection = <span class="hljs-literal">null</span>;<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">StarRocksJdbcConnConfig</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Serializable</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">long</span> <span class="hljs-variable">serialVersionUID</span> <span class="hljs-operator">=</span> <span class="hljs-number">1L</span>;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">MAX_RETRY_TIMES</span> <span class="hljs-operator">=</span> <span class="hljs-number">3</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">CONNECTION_CHECK_TIMEOUT_SECONDS</span> <span class="hljs-operator">=</span> <span class="hljs-number">5</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">DRIVER_NAME</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;com.mysql.cj.jdbc.Driver&quot;</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span>  <span class="hljs-variable">TEST_URL</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;jdbc:mysql://xxx:xxx/xxx&quot;</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span>  <span class="hljs-variable">TEST_USERNAME</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;xxx&quot;</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span>  <span class="hljs-variable">TEST_PASSWORD</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;xxx&quot;</span>;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">PROD_URL</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;jdbc:mysql://xxx:xxx/xxxx&quot;</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">PROD_USERNAME</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;xxx&quot;</span>;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">PROD_PASSWORD</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;xxx&quot;</span>;<br><br><br>&#125;<br></code></pre></td></tr></table></figure><p>EventSum UDF：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">EventSum</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">ScalarFunction</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">Logger</span> <span class="hljs-variable">LOGGER</span> <span class="hljs-operator">=</span> LogManager.getLogger(EventSum.class);<br><br>    <span class="hljs-keyword">private</span> StarRocksJdbcConnectionProvider jdbcConnProvider;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> <span class="hljs-keyword">volatile</span> PreparedStatement statement;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">QUERY</span> <span class="hljs-operator">=</span><br>            <span class="hljs-string">&quot;SELECT COUNT(`user_id`) FROM dwd_pol_user_events WHERE `credit_code` = ? AND `internal_event_code` = ?&quot;</span>;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">establishStatement</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> SQLException, ClassNotFoundException &#123;<br>        statement = jdbcConnProvider.getConnection().prepareStatement(QUERY);<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">open</span><span class="hljs-params">(FunctionContext context)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">currentEnv</span> <span class="hljs-operator">=</span> context.getJobParameter(ENV, Constant.TEST);<br>        LOGGER.info(<span class="hljs-string">&quot;Current env: &#123;&#125;&quot;</span>, currentEnv);<br>        jdbcConnProvider = <span class="hljs-keyword">new</span> <span class="hljs-title class_">StarRocksJdbcConnectionProvider</span>(currentEnv);<br>        establishStatement();<br>    &#125;<br><br>    <span class="hljs-meta">@FunctionHint(</span><br><span class="hljs-meta">            input = &#123;@DataTypeHint(&quot;STRING&quot;), @DataTypeHint(&quot;STRING&quot;)&#125;,</span><br><span class="hljs-meta">            output = @DataTypeHint(&quot;BIGINT&quot;))</span><br>    <span class="hljs-keyword">public</span> Long <span class="hljs-title function_">eval</span><span class="hljs-params">(String creditCode, String internalEventCode)</span> <span class="hljs-keyword">throws</span> SQLException &#123;<br>        <span class="hljs-type">long</span> <span class="hljs-variable">sum</span> <span class="hljs-operator">=</span> <span class="hljs-number">0L</span>;<br>        <span class="hljs-keyword">if</span> (StrUtil.isBlankIfStr(creditCode) || StrUtil.isBlankIfStr(internalEventCode)) &#123;<br>            <span class="hljs-keyword">return</span> sum;<br>        &#125;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">retry</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; retry &lt;= MAX_RETRY_TIMES; retry++) &#123;<br>            <span class="hljs-keyword">try</span> &#123;<br>                statement.clearParameters();<br>                statement.setString(<span class="hljs-number">1</span>, creditCode);<br>                statement.setString(<span class="hljs-number">2</span>, internalEventCode);<br>                <span class="hljs-keyword">try</span> (<span class="hljs-type">ResultSet</span> <span class="hljs-variable">resultSet</span> <span class="hljs-operator">=</span> statement.executeQuery()) &#123;<br>                    <span class="hljs-keyword">while</span> (resultSet.next()) &#123;<br>                        <span class="hljs-type">Object</span> <span class="hljs-variable">object</span> <span class="hljs-operator">=</span> resultSet.getObject(<span class="hljs-number">1</span>);<br>                        sum = object == <span class="hljs-literal">null</span> ? sum : (Long) object;<br>                    &#125;<br>                &#125;<br>            &#125; <span class="hljs-keyword">catch</span> (SQLException e) &#123;<br>                LOGGER.error(String.format(<span class="hljs-string">&quot;JDBC executeBatch error, retry times = %d&quot;</span>, retry), e);<br>                <span class="hljs-keyword">if</span> (retry &gt;= MAX_RETRY_TIMES) &#123;<br>                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(<span class="hljs-string">&quot;Execution of JDBC statement failed.&quot;</span>, e);<br>                &#125;<br>                <span class="hljs-keyword">try</span> &#123;<br>                    <span class="hljs-keyword">if</span> (!jdbcConnProvider.isConnectionValid()) &#123;<br>                        statement.close();<br>                        jdbcConnProvider.reestablishConnection();<br>                        establishStatement();<br>                    &#125;<br>                &#125; <span class="hljs-keyword">catch</span> (SQLException | ClassNotFoundException exception) &#123;<br>                    LOGGER.error(<br>                            <span class="hljs-string">&quot;JDBC connection is not valid, and reestablish connection failed&quot;</span>,<br>                            exception);<br>                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(<span class="hljs-string">&quot;Reestablish JDBC connection failed&quot;</span>, exception);<br>                &#125;<br><br>                <span class="hljs-keyword">try</span> &#123;<br>                    Thread.sleep(<span class="hljs-number">1000L</span> * retry);<br>                &#125; <span class="hljs-keyword">catch</span> (InterruptedException e1) &#123;<br>                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e1);<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> sum;<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> SQLException &#123;<br>        <span class="hljs-keyword">if</span> (statement != <span class="hljs-literal">null</span>) &#123;<br>            statement.close();<br>        &#125;<br>        jdbcConnProvider.close();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>获取 company 触发了某个行为的所有用户手机号码：<br>ExtractPhoneNumbers UDF：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ExtractPhoneNumbers</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">ScalarFunction</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">Logger</span> <span class="hljs-variable">LOGGER</span> <span class="hljs-operator">=</span> LogManager.getLogger(ExtractPhoneNumbers.class);<br><br>    <span class="hljs-keyword">private</span> StarRocksJdbcConnectionProvider jdbcConnProvider;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> <span class="hljs-keyword">volatile</span> PreparedStatement statement;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span> <span class="hljs-variable">QUERY</span> <span class="hljs-operator">=</span><br>            <span class="hljs-string">&quot;SELECT group_concat(phone_numbers) FROM (SELECT distinct phone_number as phone_numbers FROM dwd_pol_user_events WHERE credit_code = ? AND `internal_event_code` = ? AND phone_number &lt;&gt; &#x27;&#x27;) t1&quot;</span>;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">establishStatement</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> SQLException, ClassNotFoundException &#123;<br>        statement = jdbcConnProvider.getConnection().prepareStatement(QUERY);<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">open</span><span class="hljs-params">(FunctionContext context)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">currentEnv</span> <span class="hljs-operator">=</span> context.getJobParameter(ENV, Constant.TEST);<br>        LOGGER.info(<span class="hljs-string">&quot;Current env: &#123;&#125;&quot;</span>, currentEnv);<br>        jdbcConnProvider = <span class="hljs-keyword">new</span> <span class="hljs-title class_">StarRocksJdbcConnectionProvider</span>(currentEnv);<br>        establishStatement();<br>    &#125;<br><br>    <span class="hljs-meta">@FunctionHint(</span><br><span class="hljs-meta">            input = &#123;@DataTypeHint(&quot;STRING&quot;), @DataTypeHint(&quot;STRING&quot;)&#125;,</span><br><span class="hljs-meta">            output = @DataTypeHint(&quot;STRING&quot;))</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">eval</span><span class="hljs-params">(String creditCode, String internalEventCode)</span> <span class="hljs-keyword">throws</span> SQLException &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">phoneNumbers</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;&quot;</span>;<br>        <span class="hljs-keyword">if</span> (StrUtil.isBlankIfStr(creditCode) || StrUtil.isBlankIfStr(internalEventCode)) &#123;<br>            <span class="hljs-keyword">return</span> phoneNumbers;<br>        &#125;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">retry</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; retry &lt;= MAX_RETRY_TIMES; retry++) &#123;<br>            <span class="hljs-keyword">try</span> &#123;<br>                statement.clearParameters();<br>                statement.setString(<span class="hljs-number">1</span>, creditCode);<br>                statement.setString(<span class="hljs-number">2</span>, internalEventCode);<br>                <span class="hljs-keyword">try</span> (<span class="hljs-type">ResultSet</span> <span class="hljs-variable">resultSet</span> <span class="hljs-operator">=</span> statement.executeQuery()) &#123;<br>                    <span class="hljs-keyword">while</span> (resultSet.next()) &#123;<br>                        <span class="hljs-type">Object</span> <span class="hljs-variable">object</span> <span class="hljs-operator">=</span> resultSet.getObject(<span class="hljs-number">1</span>);<br>                        phoneNumbers = object == <span class="hljs-literal">null</span> ? phoneNumbers : (String) object;<br>                    &#125;<br>                &#125;<br>            &#125; <span class="hljs-keyword">catch</span> (SQLException e) &#123;<br>                LOGGER.error(String.format(<span class="hljs-string">&quot;JDBC executeBatch error, retry times = %d&quot;</span>, retry), e);<br>                <span class="hljs-keyword">if</span> (retry &gt;= MAX_RETRY_TIMES) &#123;<br>                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(<span class="hljs-string">&quot;Execution of JDBC statement failed.&quot;</span>, e);<br>                &#125;<br>                <span class="hljs-keyword">try</span> &#123;<br>                    <span class="hljs-keyword">if</span> (!jdbcConnProvider.isConnectionValid()) &#123;<br>                        statement.close();<br>                        jdbcConnProvider.reestablishConnection();<br>                        establishStatement();<br>                    &#125;<br>                &#125; <span class="hljs-keyword">catch</span> (SQLException | ClassNotFoundException exception) &#123;<br>                    LOGGER.error(<br>                            <span class="hljs-string">&quot;JDBC connection is not valid, and reestablish connection failed&quot;</span>,<br>                            exception);<br>                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(<span class="hljs-string">&quot;Reestablish JDBC connection failed&quot;</span>, exception);<br>                &#125;<br><br>                <span class="hljs-keyword">try</span> &#123;<br>                    Thread.sleep(<span class="hljs-number">1000L</span> * retry);<br>                &#125; <span class="hljs-keyword">catch</span> (InterruptedException e1) &#123;<br>                    <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>(e1);<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> phoneNumbers;<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> SQLException &#123;<br>        <span class="hljs-keyword">if</span> (statement != <span class="hljs-literal">null</span>) &#123;<br>            statement.close();<br>        &#125;<br>        jdbcConnProvider.close();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></li></ul><h2 id="7-SQL-代码"><a href="#7-SQL-代码" class="headerlink" title="7 SQL 代码"></a>7 SQL 代码</h2><h3 id="②-号Flink-任务"><a href="#②-号Flink-任务" class="headerlink" title="② 号Flink 任务"></a>② 号Flink 任务</h3><p>任务配置：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">-p 3<br></code></pre></td></tr></table></figure><p>SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span><br>CATALOG kafka <span class="hljs-keyword">WITH</span> (<br>    <span class="hljs-string">&#x27;type&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;hive&#x27;</span>,<br>    <span class="hljs-string">&#x27;default-database&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;bigdata_policy_rt&#x27;</span>,<br>    <span class="hljs-string">&#x27;hive-conf-dir&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;/etc/hive/conf&#x27;</span><br>);<br><br><span class="hljs-keyword">CREATE</span><br>CATALOG starrocks <span class="hljs-keyword">WITH</span> (<br>    <span class="hljs-string">&#x27;type&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;hive&#x27;</span>,<br>    <span class="hljs-string">&#x27;default-database&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;bigdata_policy_rt&#x27;</span>,<br>    <span class="hljs-string">&#x27;hive-conf-dir&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;/etc/hive/conf&#x27;</span><br>);<br><br>USE CATALOG kafka;<br>USE CATALOG starrocks;<br><br><span class="hljs-comment">-- 去重</span><br><span class="hljs-keyword">CREATE</span><br>TEMPORARY <span class="hljs-keyword">VIEW</span> IF <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> view_user_events <span class="hljs-keyword">AS</span><br><span class="hljs-keyword">SELECT</span> IFNULL(`currCreditCode`, <span class="hljs-string">&#x27;&#x27;</span>)  <span class="hljs-keyword">AS</span> `credit_code`,<br>       IFNULL(`currCompanyName`, <span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">AS</span> `company_name`,<br>       `internal_event_code`,<br>       `internal_event_name`,<br>       IFNULL(`user_id`, <span class="hljs-string">&#x27;&#x27;</span>)         <span class="hljs-keyword">AS</span> `user_id`,<br>       IFNULL(`phone`, <span class="hljs-string">&#x27;&#x27;</span>)           <span class="hljs-keyword">AS</span> `phone_number`,<br>       `<span class="hljs-type">time</span>`,<br>       TO_DATE(FROM_UNIXTIME(<span class="hljs-built_in">CAST</span>(`<span class="hljs-type">time</span>` <span class="hljs-keyword">AS</span> <span class="hljs-type">BIGINT</span>) <span class="hljs-operator">/</span> <span class="hljs-number">1000</span>)) <span class="hljs-keyword">AS</span> `<span class="hljs-type">date</span>`<br><span class="hljs-keyword">FROM</span> <span class="hljs-keyword">TABLE</span>(<br>        TUMBLE(<span class="hljs-keyword">TABLE</span> kafka.bigdata_policy_rt.dwd_pol_kafka_portrait_action_info, DESCRIPTOR(proc_time), <span class="hljs-type">INTERVAL</span> <span class="hljs-string">&#x27;60&#x27;</span> seconds))<br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> `currCreditCode`, `currCompanyName`, `internal_event_code`, `internal_event_name`, `user_id`, `phone`, `<span class="hljs-type">time</span>`;<br><br><br><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> `starrocks`.`bigdata_policy_rt`.`dwd_pol_user_events`<br><span class="hljs-keyword">SELECT</span> `credit_code`,<br>       `internal_event_code`,<br>       `user_id`,<br>       `phone_number`,<br>       TO_DATE(FROM_UNIXTIME(<span class="hljs-built_in">CAST</span>(`<span class="hljs-type">time</span>` <span class="hljs-keyword">AS</span> <span class="hljs-type">BIGINT</span>) <span class="hljs-operator">/</span> <span class="hljs-number">1000</span>)) <span class="hljs-keyword">AS</span> `<span class="hljs-type">date</span>`<br><span class="hljs-keyword">FROM</span> view_user_events<br><span class="hljs-keyword">WHERE</span> `<span class="hljs-type">date</span>` <span class="hljs-operator">&gt;=</span> <span class="hljs-string">&#x27;2023-08-07&#x27;</span>;<br><br><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> `kafka`.`bigdata_policy_rt`.`dwd_pol_kafka_clue_user_events`<br><span class="hljs-keyword">SELECT</span> `credit_code`,<br>       `company_name`,<br>       `internal_event_code`,<br>       `internal_event_name`,<br>       `user_id`,<br>       `phone_number`,<br>       `<span class="hljs-type">time</span>`<br><span class="hljs-keyword">FROM</span> view_user_events<br><span class="hljs-keyword">WHERE</span> `<span class="hljs-type">date</span>` <span class="hljs-operator">&gt;=</span> <span class="hljs-string">&#x27;2023-08-07&#x27;</span>;<br></code></pre></td></tr></table></figure><h3 id="③-号Flink-任务"><a href="#③-号Flink-任务" class="headerlink" title="③ 号Flink 任务"></a>③ 号Flink 任务</h3><p>任务配置：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">-p 1 -Dtaskmanager.numberOfTaskSlots=1 -Dtaskmanager.memory.process.size=3072m<br></code></pre></td></tr></table></figure><p>SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SET</span> <span class="hljs-string">&#x27;pipeline.global-job-parameters&#x27;</span><span class="hljs-operator">=</span><span class="hljs-string">&#x27;env:prod&#x27;</span>;<br><br><span class="hljs-keyword">CREATE</span> CATALOG kafka <span class="hljs-keyword">WITH</span> (<br>    <span class="hljs-string">&#x27;type&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;hive&#x27;</span>,<br>    <span class="hljs-string">&#x27;default-database&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;bigdata_policy_rt&#x27;</span>,<br>    <span class="hljs-string">&#x27;hive-conf-dir&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;/etc/hive/conf&#x27;</span><br>);<br><br><span class="hljs-keyword">CREATE</span> CATALOG mysql <span class="hljs-keyword">WITH</span> (<br>    <span class="hljs-string">&#x27;type&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;hive&#x27;</span>,<br>    <span class="hljs-string">&#x27;default-database&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;bigdata_policy_rt&#x27;</span>,<br>    <span class="hljs-string">&#x27;hive-conf-dir&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;/etc/hive/conf&#x27;</span><br>);<br><br><span class="hljs-keyword">CREATE</span> CATALOG starrocks <span class="hljs-keyword">WITH</span> (<br>    <span class="hljs-string">&#x27;type&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;hive&#x27;</span>,<br>    <span class="hljs-string">&#x27;default-database&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;bigdata_policy_rt&#x27;</span>,<br>    <span class="hljs-string">&#x27;hive-conf-dir&#x27;</span> <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;/etc/hive/conf&#x27;</span><br>);<br><br>USE CATALOG kafka;<br>USE CATALOG mysql;<br>USE CATALOG starrocks;<br><br><br><span class="hljs-keyword">CREATE</span> TEMPORARY <span class="hljs-keyword">FUNCTION</span> EventSum <span class="hljs-keyword">AS</span> <span class="hljs-string">&#x27;xxx.EventSum&#x27;</span>;<br><span class="hljs-keyword">CREATE</span> TEMPORARY <span class="hljs-keyword">FUNCTION</span> ExtractPhoneNumbers <span class="hljs-keyword">AS</span> <span class="hljs-string">&#x27;xxx.ExtractPhoneNumbers&#x27;</span>;<br><br><span class="hljs-keyword">CREATE</span> TEMPORARY <span class="hljs-keyword">VIEW</span> IF <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> view_company_event_info <span class="hljs-keyword">AS</span><br><span class="hljs-keyword">SELECT</span> `credit_code`,<br>       `company_name`,<br>       `internal_event_code`,<br>       `internal_event_name`,<br>       `user_id`,<br>       `phone_number`,<br>       `<span class="hljs-type">time</span>`<br><span class="hljs-keyword">FROM</span> <span class="hljs-keyword">TABLE</span>(<br>        TUMBLE(<span class="hljs-keyword">TABLE</span> `kafka`.`bigdata_policy_rt`.`dwd_pol_kafka_clue_user_events`, DESCRIPTOR(proc_time), <span class="hljs-type">INTERVAL</span> <span class="hljs-string">&#x27;60&#x27;</span> seconds))<br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> `credit_code`,<br>         `company_name`,<br>         `internal_event_code`,<br>         `internal_event_name`,<br>         `user_id`,<br>         `phone_number`,<br>         `<span class="hljs-type">time</span>`;<br><br><span class="hljs-comment">-- 筛选出触发了阈值和未创建线索的企业</span><br><span class="hljs-keyword">CREATE</span> TEMPORARY <span class="hljs-keyword">VIEW</span> IF <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> view_clue_trigger_company <span class="hljs-keyword">AS</span><br><span class="hljs-keyword">SELECT</span> t1.credit_code,<br>       t1.company_name,<br>       t1.internal_event_code,<br>       t1.internal_event_name,<br>       t1.event_sum <span class="hljs-keyword">AS</span> event_trigger_num,<br>       t1.proc_time<br><span class="hljs-keyword">FROM</span> (<br>         <span class="hljs-keyword">SELECT</span><br>             credit_code,<br>             company_name,<br>             internal_event_code,<br>             internal_event_name,<br>             EventSum(credit_code, internal_event_code) <span class="hljs-keyword">AS</span> event_sum,<br>             PROCTIME() <span class="hljs-keyword">AS</span> proc_time<br>         <span class="hljs-keyword">FROM</span> view_company_event_info<br>     ) t1<br><span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">JOIN</span> `mysql`.`bigdata_policy_rt`.`dim_pol_clue_event_threshold` <span class="hljs-comment">/*+ OPTIONS(&#x27;lookup.cache&#x27;=&#x27;PARTIAL&#x27;, &#x27;lookup.cache.max-rows&#x27;=&#x27;5000&#x27;, &#x27;lookup.cache.ttl&#x27;=&#x27;10m&#x27;, &#x27;lookup.partial-cache.expire-after-write&#x27;=&#x27;10m&#x27;, &#x27;lookup.partial-cache.expire-after-access&#x27;=&#x27;10m&#x27;) */</span> <span class="hljs-keyword">FOR</span> <span class="hljs-built_in">SYSTEM_TIME</span> <span class="hljs-keyword">AS</span> <span class="hljs-keyword">OF</span> t1.proc_time <span class="hljs-keyword">AS</span> t2<br><span class="hljs-keyword">ON</span> t1.internal_event_code <span class="hljs-operator">=</span> t2.internal_event_code<br><span class="hljs-keyword">WHERE</span> t2.internal_event_code <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">and</span> t1.event_sum <span class="hljs-operator">&gt;=</span> t2.threshold_num;<br><br><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> `mysql`.`bigdata_policy_rt`.`dwd_pol_clue_trigger_info`<br><span class="hljs-keyword">SELECT</span> credit_code,<br>       company_name,<br>       ExtractPhoneNumbers(credit_code, internal_event_code) <span class="hljs-keyword">AS</span> phone_numbers,<br>       internal_event_code,<br>       internal_event_name,<br>       event_trigger_num<br><span class="hljs-keyword">FROM</span> (<br>         <span class="hljs-keyword">SELECT</span> t1.credit_code,<br>                t1.company_name,<br>                t1.internal_event_code,<br>                t1.internal_event_name,<br>                t1.event_trigger_num<br>         <span class="hljs-keyword">FROM</span> view_clue_trigger_company t1<br>                  <span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">JOIN</span> `mysql`.`bigdata_policy_rt`.`dwd_pol_clue_trigger_info` <span class="hljs-keyword">FOR</span> <span class="hljs-built_in">SYSTEM_TIME</span> <span class="hljs-keyword">AS</span> <span class="hljs-keyword">OF</span> t1.proc_time <span class="hljs-keyword">AS</span> t2<br>                            <span class="hljs-keyword">ON</span> t1.credit_code <span class="hljs-operator">=</span> t2.credit_code<br>         <span class="hljs-keyword">WHERE</span> t2.credit_code <span class="hljs-keyword">IS</span> <span class="hljs-keyword">NULL</span><br>     ) t3;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Flink 实时项目</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Flink + StarRocks</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
